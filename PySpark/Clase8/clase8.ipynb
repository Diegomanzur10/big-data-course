{"cells": [{"cell_type": "markdown", "id": "61daad76-a9c0-4762-9cba-608343d67e3f", "metadata": {}, "source": "## Spark SQL es muy f\u00e1cil de usar, y punto. Quiz\u00e1 ya sepas que tambi\u00e9n es bastante dif\u00edcil de dominar.\n\nPara ser competente en Spark, hay que tener tres habilidades fundamentales:\n\n-La capacidad de manipular y entender los datos\n-El conocimiento de c\u00f3mo adaptar la herramienta a las necesidades del programador\n-El arte de encontrar un equilibrio entre los factores que afectan a las ejecuciones de los trabajos de Spark\n\nHe elaborado los siguientes seis ejercicios que se asemejan a algunas situaciones t\u00edpicas a las que los desarrolladores de Spark se enfrentan a diario cuando construyen sus pipelines: estos ayudar\u00e1n a evaluar las habilidades anteriores.\n\n### El conjunto de datos\nDescribamos brevemente el conjunto de datos que vamos a utilizar: consta de tres tablas procedentes de la base de datos de una tienda, con productos, ventas y vendedores. Los datos est\u00e1n disponibles en archivos Parquet\n\n\n<center><img src=\"https://miro.medium.com/max/700/1*wA4xJu3LMcm_vR5pFJkLpA.png\" width=500 height=500 />\n    \n\n"}, {"cell_type": "markdown", "id": "864bfed4-9d24-4c7f-bd64-30e59b21ffd8", "metadata": {}, "source": "### Tabla de ventas\nCada fila de esta tabla es un pedido y cada pedido puede contener s\u00f3lo un producto. Cada fila almacena los siguientes campos:\n\n- order_id: El ID del pedido\n- product_id: El \u00fanico producto vendido en el pedido. Todos los pedidos tienen exactamente un producto)\n- seller_id: El ID del empleado vendedor que vendi\u00f3 el producto\n- num_pieces_sold: El n\u00famero de unidades vendidas para el producto espec\u00edfico en el pedido\n- bill_raw_text: Una cadena que representa el texto en bruto de la factura asociada al pedido\n- date: La fecha del pedido.\n\n### Tabla de productos\nCada fila representa un producto distinto. Los campos son\n\n- product_id: El ID del producto\n- product_name: El nombre del producto\n- price: El precio del producto\n\n### Tabla de vendedores\nEsta tabla contiene la lista de todos los vendedores:\n\n- seller_id: El ID del vendedor\n- seller_name: El nombre del vendedor\n- daily_target: El n\u00famero de art\u00edculos (independientemente del tipo de producto) que el vendedor necesita para alcanzar su cuota. Por ejemplo, si el objetivo diario es 100.000, el empleado necesita vender 100.000 productos, puede alcanzar la cuota vendiendo 100.000 unidades del producto_0, pero tambi\u00e9n vendiendo 30.000 unidades del producto_1 y 70.000 unidades del producto_2\n\n\n### Ejercicios\nLa mejor manera de aprovechar los ejercicios que se presentan a continuaci\u00f3n es obtener los datos e implementar un c\u00f3digo de trabajo que resuelva los problemas propuestos.\n\nConsejo: He construido el conjunto de datos para poder trabajar en una sola m\u00e1quina: cuando escribas el c\u00f3digo, imagina lo que pasar\u00eda con un conjunto de datos 100 veces mayor.\n\nAunque sepas c\u00f3mo resolverlas, \u00a1mi consejo es que no te saltes las preguntas de calentamiento! (si sabes que Spark te llevar\u00e1 unos segundos).\n"}, {"cell_type": "code", "execution_count": 1, "id": "d1d24e3e-0c5f-4bb2-bda3-1cfca0ce90b7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.3.2)\nCollecting pyspark-stubs\n  Downloading pyspark_stubs-3.0.0.post3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\nCollecting pyspark\n  Downloading pyspark-3.0.3.tar.gz (209.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.1/209.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9 (from pyspark)\n  Downloading py4j-0.10.9-py2.py3-none-any.whl.metadata (1.3 kB)\nDownloading pyspark_stubs-3.0.0.post3-py3-none-any.whl (110 kB)\nDownloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\nBuilding wheels for collected packages: pyspark\n\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.3-py2.py3-none-any.whl size=209435952 sha256=2c99e5638ef4cd1abe2b363f03494feeb50a0b14238cfb4c792de10861b9ccef\n  Stored in directory: /root/.cache/pip/wheels/40/50/14/79047c3c171b701e591d287b78a201214d9c8e0b93fef64458\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark, pyspark-stubs\n\u001b[2K  Attempting uninstall: py4j\n\u001b[2K    Found existing installation: py4j 0.10.9.5\n\u001b[2K    Uninstalling py4j-0.10.9.5:\n\u001b[2K      Successfully uninstalled py4j-0.10.9.5\n\u001b[2K  Attempting uninstall: pyspark\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0/3\u001b[0m [py4j]\n\u001b[2K    Found existing installation: pyspark 3.3.20m \u001b[32m0/3\u001b[0m [py4j]\n\u001b[2K    Uninstalling pyspark-3.3.2:\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0/3\u001b[0m [py4j]\n\u001b[2K      Successfully uninstalled pyspark-3.3.2\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/3\u001b[0m [pyspark]\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3/3\u001b[0m [pyspark-stubs]0m [pyspark]\n\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9 pyspark-3.0.3 pyspark-stubs-3.0.0.post3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "# Pyspark\n!pip install pyspark pyspark-stubs"}, {"cell_type": "markdown", "id": "8ac147e9-dd84-4c31-9627-556b1969ffb5", "metadata": {}, "source": "### 0.1 Averig\u00fce cu\u00e1ntos pedidos, cu\u00e1ntos productos y cu\u00e1ntos vendedores hay en los datos.\n\n\u00bfCu\u00e1ntos productos se han vendido al menos una vez? \u00bfCu\u00e1l es el producto que contiene m\u00e1s pedidos?\n\nCrea la sesi\u00f3n de Spark y lea los archivos con el siguiente c\u00f3digo\n\n```\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n    .config(\"spark.files.overwrite\", \"true\") \\\n    .appName(\"Exercise1\") \\\n    .getOrCreate()\n\n\nproducts = spark.read.csv(\n   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsellers = spark.read.csv(\n   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsales = spark.read.csv(\n   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n)\n```\n    \n\n    "}, {"cell_type": "markdown", "id": "f2a5e9b0-aaec-4356-bf90-e32e00173a98", "metadata": {}, "source": "### 0.2 \u00bfCu\u00e1ntos productos distintos se han vendido en cada d\u00eda?\n\nCree la sesi\u00f3n de Spark utilizando el siguiente c\u00f3digo\n\n```\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n    .config(\"spark.files.overwrite\", \"true\") \\\n    .appName(\"Exercise1\") \\\n    .getOrCreate()\n\n\nproducts = spark.read.csv(\n   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsellers = spark.read.csv(\n   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsales = spark.read.csv(\n   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n)\n```"}, {"cell_type": "code", "execution_count": null, "id": "fae53ad8-b16b-4f2c-b226-22ce66aa01e8", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "d6029b64-3769-4072-9c37-6f656f2e81cb", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "4d8ca08f-cb6f-4696-a203-5b6cd397bfe0", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "391a8f98-6f11-4d0f-81f2-bdc72186efee", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "8c95e491-4b87-4e62-96de-2e752d6a3017", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "5ef68608-971c-4c47-825b-26588dee5130", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "9fd0debd-2df0-4abe-b3d8-1a8fbc9ccfba", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "a57a857c-9727-476a-b5e8-6cf58fdfadd2", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "d3959890-240f-42ba-a15c-fdd103739f2d", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "d888392b-01b2-4fe1-b196-7bcc5f67e0f2", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "6b87b54e-9349-4fa3-8c36-a8d8e992f17c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "a07fb0d4-3b1c-46fa-ae7a-6410a82a9885", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "40e56055-1886-45a8-bb70-5f8af8a5435c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "8d4741c4-b3e2-40b6-a464-e0c1151562ff", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "61badb22-0631-475e-97a7-f11da09403bc", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "204644ee-0221-4d71-86fc-d77eeba81751", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "30d6f81a-e1c7-40e1-bf0c-1a4422ee0d8b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "3cd7a134-d548-4bf2-80bf-a1ca16550549", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "240ed003-64a1-4045-9601-5d38d778b6cd", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "48c7a5d1-d5c7-4e70-94f0-872693276f64", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "5e2878ba-c937-4cc2-8ba9-f03e12f8d64b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "ae666dbd-9a2d-4a04-869d-ee811368a137", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "a6aaa428-eb94-4fb1-a7fa-79c8c74d8057", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 4, "id": "2e5c13b2-a1a1-40db-ab6c-f6753d077e65", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/28 20:54:50 INFO SparkEnv: Registering MapOutputTracker\n25/05/28 20:54:50 INFO SparkEnv: Registering BlockManagerMaster\n25/05/28 20:54:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/05/28 20:54:50 INFO SparkEnv: Registering OutputCommitCoordinator\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Number of Orders: 500001\nNumber of sellers: 10\nNumber of products: 40000\n"}], "source": "#Soluci\u00f3n ejercicio 0.1\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n    .config(\"spark.files.overwrite\", \"true\") \\\n    .appName(\"Exercise1\") \\\n    .getOrCreate()\n\n\nproducts = spark.read.csv(\n   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsellers = spark.read.csv(\n   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsales = spark.read.csv(\n   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\n#   Print the number of orders\nprint(\"Number of Orders: {}\".format(sales.count()))\n\n#   Print the number of sellers\nprint(\"Number of sellers: {}\".format(sellers.count()))\n\n#   Print the number of products\nprint(\"Number of products: {}\".format(products.count()))\n\nspark.stop()"}, {"cell_type": "code", "execution_count": 3, "id": "0a524edd-85c7-4ac1-b913-6cc4b7c8dd55", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+----------------------+\n|      date|distinct_products_sold|\n+----------+----------------------+\n|2020-07-07|                  2500|\n|2020-07-01|                  2478|\n|2020-07-09|                  2469|\n|2020-07-08|                  2439|\n|2020-07-04|                  2429|\n|2020-07-05|                  2419|\n|2020-07-10|                  2403|\n|2020-07-02|                  2391|\n|2020-07-03|                  2367|\n|2020-07-06|                  2330|\n+----------+----------------------+\n\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n    .config(\"spark.executor.memory\", \"500mb\") \\\n    .appName(\"Exercise1\") \\\n    .getOrCreate()\n\n# Read Source tables\nproducts = spark.read.csv(\n   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsellers = spark.read.csv(\n   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsales = spark.read.csv(\n   'file:////clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n)\n\nsales.groupby(col(\"date\")).agg(countDistinct(col(\"product_id\")).alias(\"distinct_products_sold\")).orderBy(\n    col(\"distinct_products_sold\").desc()).show()\n\nspark.stop()"}, {"cell_type": "markdown", "id": "cfcbd689-fd34-4713-8801-c3f950770d09", "metadata": {}, "source": "## Comparativa entre Spark y Pandas"}, {"cell_type": "code", "execution_count": 6, "id": "2caf1ba6-13ea-492c-bedc-f2479acf0658", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[pandas]   Products=4000000, Sellers=100, Sales=500000\n[pandas]   Elapsed time: 8.64 s\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/05/28 22:02:56 INFO SparkEnv: Registering MapOutputTracker\n25/05/28 22:02:56 INFO SparkEnv: Registering BlockManagerMaster\n25/05/28 22:02:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/05/28 22:02:57 INFO SparkEnv: Registering OutputCommitCoordinator\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[PySpark]  Products=4000000, Sellers=100, Sales=500000\n[PySpark]  Elapsed time: 7.67 s\n"}], "source": "import time\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\ndef benchmark_pandas():\n    start = time.perf_counter()\n\n    # Read CSVs into pandas\n    products = pd.read_csv('/clase8/products.csv')\n    sellers  = pd.read_csv('/clase8/sellers.csv')\n    sales    = pd.read_csv('/clase8/sales.csv')\n\n    # Compute counts\n    n_products = len(products)\n    n_sellers  = len(sellers)\n    n_sales    = len(sales)\n\n    elapsed = time.perf_counter() - start\n    return elapsed, (n_products, n_sellers, n_sales)\n\ndef benchmark_spark():\n    start = time.perf_counter()\n\n    # Start SparkSession\n    spark = SparkSession.builder \\\n        .master(\"local\") \\\n        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n        .config(\"spark.files.overwrite\", \"true\") \\\n        .appName(\"Benchmark\") \\\n        .getOrCreate()\n\n    # Read CSVs into DataFrames\n    products = spark.read.csv('file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\")\n    sellers  = spark.read.csv('file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\")\n    sales    = spark.read.csv('file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\")\n\n    # Trigger full scan counts\n    n_sales    = sales.count()\n    n_sellers  = sellers.count()\n    n_products = products.count()\n\n    spark.stop()\n\n    elapsed = time.perf_counter() - start\n    return elapsed, (n_products, n_sellers, n_sales)\n\ndef main():\n    pd_time, (pd_prod, pd_seller, pd_sales) = benchmark_pandas()\n    print(f\"[pandas]   Products={pd_prod}, Sellers={pd_seller}, Sales={pd_sales}\")\n    print(f\"[pandas]   Elapsed time: {pd_time:.2f} s\\n\")\n\n    spark_time, (sp_prod, sp_seller, sp_sales) = benchmark_spark()\n    print(f\"[PySpark]  Products={sp_prod}, Sellers={sp_seller}, Sales={sp_sales}\")\n    print(f\"[PySpark]  Elapsed time: {spark_time:.2f} s\")\n\nif __name__ == \"__main__\":\n    main()\n"}, {"cell_type": "code", "execution_count": null, "id": "061beaa6-89e9-49ff-82d1-d9d23831c2e0", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}