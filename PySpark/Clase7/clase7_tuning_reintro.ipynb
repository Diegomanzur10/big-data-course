{"cells": [{"cell_type": "markdown", "id": "bdf96acb-348a-48ed-a4a8-fd8a3405f441", "metadata": {}, "source": "<a id='afinando-un-trabajo-de-spark'></a>\n## Ajuste fino de un trabajo de Spark"}, {"cell_type": "markdown", "id": "1123cc78-f9c0-4b90-8199-d1677275e87b", "metadata": {}, "source": "Antes de empezar, ten en cuenta que toda esta secci\u00f3n est\u00e1 escrita exclusivamente en base a la experiencia. Puede diferir con los casos de uso, pero le ayudar\u00e1 a tener una mejor comprensi\u00f3n de lo que debe buscar, o actuar como una gu\u00eda para lograr su objetivo.\n\n>El ajuste del rendimiento de Spark se refiere al proceso de ajustar la configuraci\u00f3n para registrar la memoria, los n\u00facleos y las instancias utilizadas por el sistema. Este proceso garantiza que Spark tenga un rendimiento impecable y tambi\u00e9n evita el cuello de botella de los recursos en Spark.\n\nTeniendo en cuenta que est\u00e1s utilizando Amazon EMR para ejecutar tus trabajos de Spark, hay tres aspectos que debes cuidar:\n1. Dimensionamiento de EMR\n2. Configuraciones de Spark\n3. Ajuste de trabajos\n"}, {"cell_type": "markdown", "id": "9a3d8ceb-7f0e-49e0-9edd-181754b92354", "metadata": {}, "source": "<a id='emr-sizing'></a>\n### EMR/Dataproc Sizing"}, {"cell_type": "markdown", "id": "327e2f4f-3bb9-438e-a029-ab87ad6c9a7e", "metadata": {}, "source": "Dimensionar tu EMR es extremadamente importante, ya que esto afecta a la eficiencia de tus trabajos en Spark. Aparte del factor del coste, el n\u00famero m\u00e1ximo de nodos y de memoria que su trabajo puede utilizar se decidir\u00e1 por esto. Si se hace trabajar un EMR con altas especificaciones, obviamente significa que se est\u00e1 pagando m\u00e1s por \u00e9l, por lo que lo ideal es utilizarlo al m\u00e1ximo. Estas son las pautas que sigo para asegurarme de que el EMR est\u00e1 correctamente dimensionado:\n\n1. Tama\u00f1o de los datos de entrada (incluye todos los datos de entrada) en el disco.\n2. Si los trabajos tienen transformaciones o simplemente pasan directamente. Eval\u00fae las uniones y las uniones complejas involucradas.\n3. Tama\u00f1o de los datos de salida en el disco.\n\nMire los criterios anteriores contra la memoria que necesita procesar y el espacio en disco que necesitar\u00eda. Comience con una configuraci\u00f3n peque\u00f1a y siga agregando nodos para llegar a una configuraci\u00f3n \u00f3ptima. En caso de que se pregunte sobre el factor *Tiempo de ejecuci\u00f3n frente a la configuraci\u00f3n de EMR*, comprenda que est\u00e1 bien que un trabajo se ejecute durante m\u00e1s tiempo, en lugar de agregar m\u00e1s recursos al cl\u00faster. Por ejemplo, est\u00e1 bien ejecutar un trabajo durante 40 minutos en un cl\u00faster de 5 nodos, en lugar de ejecutar un trabajo en 10 minutos en un cl\u00faster de 15 nodos.\n"}, {"cell_type": "markdown", "id": "9a04c3cf-dfc7-4b66-9964-b0fcb4c848c8", "metadata": {}, "source": "Otra cosa que debe saber sobre los EMR son los diferentes tipos de instancias EC2 proporcionados por Amazon. Hablar\u00e9 brevemente sobre ellos, pero le recomiendo que lea m\u00e1s sobre ellos en la [documentaci\u00f3n oficial] (https://aws.amazon.com/ec2/instance-types/). Hay 5 tipos de clases de instancia. Seg\u00fan el trabajo que desee ejecutar, puede decidir cu\u00e1l usar:\n\n>Clase de instancia | Descripci\u00f3n\n>--- | ---\n>Prop\u00f3sito general | Equilibrio de recursos inform\u00e1ticos, de memoria y de red\n> Computaci\u00f3n optimizada | Ideal para aplicaciones inform\u00e1ticas que se benefician de procesadores de alto rendimiento\n>Memoria optimizada | Dise\u00f1ado para ofrecer un rendimiento r\u00e1pido para cargas de trabajo que procesan grandes conjuntos de datos en la memoria\n>Almacenamiento optimizado | Para cargas de trabajo que requieren un alto acceso de lectura y escritura secuencial a conjuntos de datos muy grandes en el almacenamiento local\n>Instancias de GPU | Use aceleradores de hardware, o coprocesadores, para realizar funciones de alta demanda, de manera m\u00e1s eficiente de lo que es posible en el software que se ejecuta en las CPU.\n\nLa configuraci\u00f3n (memoria, almacenamiento, CPU, rendimiento de la red) diferir\u00e1 seg\u00fan la clase de instancia que elija.<br>\nPara ayudar a hacer la vida m\u00e1s f\u00e1cil, esto es lo que hago cuando me encuentro en un aprieto sobre cu\u00e1l elegir: <br>\n 1. Visite [ec2instances](https://www.ec2instances.info/) [gcpInstances](https://gcpinstances.doit-intl.com/)\n 2. Elija las instancias EC2 en cuesti\u00f3n\n 3. Haga clic en comparar seleccionado\n\n\u00a1Esto lo ayudar\u00e1 f\u00e1cilmente a comprender en qu\u00e9 se est\u00e1 metiendo y, por lo tanto, lo ayudar\u00e1 a tomar la mejor decisi\u00f3n! El sitio fue creado por [Garret Heaton](https://github.com/powdahound)(fundador de Swoot)."}, {"cell_type": "markdown", "id": "a8000791-bf8d-4878-94b5-6ef321fcbedf", "metadata": {}, "source": "<a id='spark-configurations'></a>\n### Spark Configurations\n\nHay un mont\u00f3n de [configuraciones] (https://spark.apache.org/docs/latest/configuration.html) que puede modificar cuando se trata de Spark. Aqu\u00ed, anotar\u00e9 algunas de las configuraciones que uso, que me han funcionado bien."}, {"cell_type": "markdown", "id": "2061b3c2-629f-41fc-a1d3-a1a89f043731", "metadata": {}, "source": "#### Job Scheduling"}, {"cell_type": "markdown", "id": "9b50bc61-3ce6-4776-b7b4-6d4f733d9065", "metadata": {}, "source": "Cuando env\u00ede su trabajo en un cl\u00faster, se entregar\u00e1 a Spark Schedulers, que es responsable de materializar un plan l\u00f3gico para su trabajo. Hay dos tipos de [programaci\u00f3n de trabajos](https://spark.apache.org/docs/latest/job-scheduling.html):\n1. FIFO<br>\nDe forma predeterminada, el programador de Spark ejecuta trabajos en modo FIFO. Cada trabajo se divide en etapas (por ejemplo, mapear y reducir fases), y el primer trabajo tiene prioridad en todos los recursos disponibles mientras que sus etapas tienen tareas para iniciar, luego el segundo trabajo tiene prioridad, etc. no es necesario usar todo el cl\u00faster, los trabajos posteriores pueden comenzar a ejecutarse de inmediato, pero si los trabajos al principio de la cola son grandes, es posible que los trabajos posteriores se retrasen significativamente.\n2. FAIR<br>\nEl programador FAIR admite la agrupaci\u00f3n de trabajos en grupos y la configuraci\u00f3n de diferentes opciones de programaci\u00f3n (por ejemplo, peso) para cada grupo. Esto puede ser \u00fatil para crear un grupo de alta prioridad para trabajos m\u00e1s importantes, por ejemplo, o para agrupar los trabajos de cada usuario y otorgarles partes iguales a los usuarios, independientemente de cu\u00e1ntos trabajos simult\u00e1neos tengan, en lugar de darles partes iguales a los trabajos. Este enfoque sigue el modelo de Hadoop Fair Scheduler.\n\n> Personalmente prefiero usar el modo FAIR, y esto se puede configurar agregando `.config(\"spark.scheduler.mode\", \"FAIR\")` cuando creas tu SparkSession."}, {"cell_type": "markdown", "id": "c7e23d5e-0795-4620-b29f-0245e6bff4a7", "metadata": {}, "source": "#### Serializer\n\nTenemos dos tipos de [serializadores](https://spark.apache.org/docs/latest/tuning.html#data-serialization) disponibles:\n1. Serializaci\u00f3n de Java\n2. Serializaci\u00f3n de Kryo\n\nKryo es significativamente m\u00e1s r\u00e1pido y m\u00e1s compacto que la serializaci\u00f3n de Java (a menudo hasta 10x), pero no es compatible con todos los tipos serializables y requiere que registre las clases que usar\u00e1 en el programa con anticipaci\u00f3n para obtener el mejor rendimiento.\n\nLa serializaci\u00f3n de Java se usa de forma predeterminada porque si tiene una clase personalizada que ampl\u00eda Serializable, se puede usar f\u00e1cilmente. Tambi\u00e9n puede controlar el rendimiento de su serializaci\u00f3n m\u00e1s de cerca extendiendo java.io.Externalizable\n\n> La recomendaci\u00f3n general es usar Kyro como serializador siempre que sea posible, ya que conduce a tama\u00f1os mucho m\u00e1s peque\u00f1os que la serializaci\u00f3n de Java. Se puede agregar usando `.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")` cuando crea su SparkSession."}, {"cell_type": "markdown", "id": "58766b9a-c7a9-44f5-a856-a3e1e12e4062", "metadata": {}, "source": "#### Shuffle Behaviour\n\nPor lo general, es una buena idea comprimir el archivo de salida despu\u00e9s de la fase de mapeo. La propiedad `spark.shuffle.compress` decide si hacer la compresi\u00f3n o no. La compresi\u00f3n utilizada es `spark.io.compression.codec`.\n\n> La propiedad se puede agregar usando `.config(\"spark.shuffle.compress\", \"true\")` cuando crea su SparkSession."}, {"cell_type": "markdown", "id": "a6d451cc-1d02-4fa9-bbb8-af1b3ff56ecb", "metadata": {}, "source": "#### Compression and Serialization\n\nHay 4 c\u00f3decs predeterminados que Spark proporciona para comprimir datos internos, como particiones RDD, registro de eventos, variables de transmisi\u00f3n y salidas aleatorias. Est\u00e1n:\n\n1. lz4\n2. lzf\n3. snappy\n4. zstd\n\n> La decisi\u00f3n sobre cu\u00e1l usar se basa en el caso de uso. Generalmente uso la compresi\u00f3n `snappy`. Google cre\u00f3 Snappy porque necesitaba algo que ofreciera una compresi\u00f3n muy r\u00e1pida a expensas del tama\u00f1o final. Snappy es r\u00e1pido, estable y gratuito, pero aumenta el tama\u00f1o m\u00e1s que los otros c\u00f3decs. Al mismo tiempo, dado que los costos de c\u00f3mputo ser\u00e1n menores, parece una compensaci\u00f3n equilibrada. La propiedad se puede agregar usando `.config(\"spark.io.compression.codec\", \"snappy\")` cuando crea su SparkSession.\n\nEsta [sesi\u00f3n](https://databricks.com/session/best-practice-of-compression-decompression-codes-in-apache-spark) explica las mejores pr\u00e1cticas de compresi\u00f3n/descompresi\u00f3n de c\u00f3digos en Apache Spark. Les recomiendo que le echen un vistazo antes de tomar una decisi\u00f3n."}, {"cell_type": "markdown", "id": "b2de0e26-ecb1-477d-8136-6e08e1a175eb", "metadata": {}, "source": "#### Scheduling\n\nLa propiedad `spark.speculation` realiza la ejecuci\u00f3n especulativa de tareas. Esto significa que si una o m\u00e1s tareas se ejecutan lentamente en una etapa, se volver\u00e1n a iniciar. La ejecuci\u00f3n especulativa no detendr\u00e1 la tarea de ejecuci\u00f3n lenta, pero inicia la nueva tarea en paralelo.\n\n> Usualmente deshabilito esta opci\u00f3n agregando `.config(\"spark.speculation\", \"false\") ` cuando creo SparkSession."}, {"cell_type": "markdown", "id": "5da336a3-d9e5-4353-a500-282e626d006a", "metadata": {}, "source": "#### Application Properties\n\nExisten principalmente dos propiedades de la aplicaci\u00f3n que debe conocer:\n\n1. spark.driver.memoryOverhead: la cantidad de memoria fuera del mont\u00f3n que se asignar\u00e1 por controlador en modo de cl\u00faster, en MiB, a menos que se especifique lo contrario. Esta es la memoria que representa cosas como los gastos generales de VM, cadenas internas, otros gastos generales nativos, etc. Esto tiende a crecer con el tama\u00f1o del contenedor (t\u00edpicamente 6-10%). Esta opci\u00f3n actualmente es compatible con YARN y Kubernetes.\n\n2. spark.executor.memoryOverhead: la cantidad de memoria fuera del mont\u00f3n que se asignar\u00e1 por ejecutor, en MiB, a menos que se especifique lo contrario. Esta es la memoria que tiene en cuenta cosas como gastos generales de VM, cadenas internas, otros gastos generales nativos, etc. Esto tiende a crecer con el tama\u00f1o del ejecutor (t\u00edpicamente 6-10%). Esta opci\u00f3n actualmente es compatible con YARN y Kubernetes.\n\n> Si alguna vez se enfrenta a un problema como `Contenedor eliminado por YARN por exceder los l\u00edmites de memoria`, sepa que se debe a que no ha especificado suficiente sobrecarga de memoria para que su trabajo se ejecute con \u00e9xito. El valor predeterminado para Overhead es 10% de la memoria disponible"}, {"cell_type": "markdown", "id": "5c319162-3238-4585-8d58-33de96cbdb64", "metadata": {}, "source": "<a id='job-tuning'></a>\n### Job Tuning"}, {"cell_type": "markdown", "id": "0fd728fc-c752-4509-bb2d-a1fddd3eff67", "metadata": {}, "source": "Adem\u00e1s del ajuste de EMR y Spark, hay otra forma de abordar las optimizaciones, y es ajustando su propio trabajo para producir resultados de manera eficiente. Repasar\u00e9 algunas de esas t\u00e9cnicas que te ayudar\u00e1n a lograrlo. La [Gu\u00eda de programaci\u00f3n de Spark](https://spark.apache.org/docs/2.1.1/programming-guide.html) habla m\u00e1s sobre estos conceptos en detalle. Si prefieren ver un video en lugar de leer, les recomiendo [A Deep Dive into Proper Optimization for Spark Jobs](https://youtu.be/daXEp4HmS-E) de Daniel Tomes de Databricks, que encontr\u00e9 realmente \u00fatil e informativo. !"}, {"cell_type": "markdown", "id": "1463b060-ce22-42c8-b5f6-394d2a913546", "metadata": {}, "source": "#### Broadcast Joins (Broadcast Hash Join)"}, {"cell_type": "markdown", "id": "2afd904f-b65c-44a0-bd21-0e4909a706ab", "metadata": {}, "source": "Para algunos trabajos, la eficiencia se puede aumentar almacen\u00e1ndolos en memoria cach\u00e9. Broadcast Hash Join (BHJ) es una t\u00e9cnica de este tipo que lo ayudar\u00e1 a optimizar las consultas de uni\u00f3n cuando el tama\u00f1o de un lado de los datos es bajo.\n>Las uniones de BroadCast son las m\u00e1s r\u00e1pidas, pero el inconveniente es que consumir\u00e1 m\u00e1s memoria tanto en el ejecutor como en el controlador.\n\nLos siguientes pasos brindan un adelanto de c\u00f3mo funciona, lo que lo ayudar\u00e1 a comprender los casos de uso en los que se puede usar:<br>\n1. El archivo de entrada (el m\u00e1s peque\u00f1o de las dos tablas) que se transmitir\u00e1 es le\u00eddo por los ejecutores en paralelo en su memoria de trabajo.\n2. Todos los datos de los ejecutores se recopilan en el controlador (por lo tanto, la necesidad de mayor memoria en el controlador).\n3. Luego, el controlador transmite el conjunto de datos combinado (copia completa) a cada ejecutor.\n4. El tama\u00f1o del conjunto de datos transmitido podr\u00eda ser varias (10-20+) veces mayor que la entrada en la memoria debido a factores como la deserializaci\u00f3n.\n5. Los ejecutores terminar\u00e1n almacenando las partes que leyeron primero, y tambi\u00e9n la copia completa, lo que genera un alto requerimiento de memoria.\n\n\n<center><img src=\"https://media-exp2.licdn.com/dms/image/C5612AQGo4w2cEWUDCg/article-inline_image-shrink_1000_1488/0/1554384116825?e=1662595200&v=beta&t=TQxovk8O4_rW74QJMKvDFQcchmVz4mLouD9fR1AfR1c\">"}, {"cell_type": "markdown", "id": "02222943-5c32-4996-a624-4a4ecef7ec40", "metadata": {}, "source": "#### Spark Partitions"}, {"cell_type": "markdown", "id": "6c484363-246b-4e99-afce-c573fbced705", "metadata": {}, "source": "Una partici\u00f3n en Spark es un fragmento at\u00f3mico de datos (divisi\u00f3n l\u00f3gica de datos) almacenado en un nodo del cl\u00faster. Las particiones son las unidades b\u00e1sicas de paralelismo en Spark. Tener un n\u00famero demasiado grande de particiones o muy pocas no es una soluci\u00f3n ideal. La cantidad de particiones en Spark debe decidirse en funci\u00f3n de la configuraci\u00f3n del cl\u00faster y los requisitos de la aplicaci\u00f3n. Aumentar el n\u00famero de particiones har\u00e1 que cada partici\u00f3n tenga menos datos o no tenga datos. En general, la partici\u00f3n de Spark se puede dividir de tres maneras:\n\n1. Input\n2. Shuffle\n3. Output\n\n##### Input\n\nSpark generalmente hace un buen trabajo al calcular la configuraci\u00f3n ideal para este, excepto en casos muy particulares. Es recomendable utilizarlo por defecto a menos que:\n1. Aumentar el paralelismo\n2. Datos muy anidados\n3. Generaci\u00f3n de datos (explotar)\n4. La fuente no es \u00f3ptima\n5. Est\u00e1s usando UDF\n\n`spark.sql.files.maxpartitionBytes`: esta propiedad indica el n\u00famero m\u00e1ximo de bytes para empaquetar en una sola partici\u00f3n al leer archivos (predeterminado 128 MB). Use esto para aumentar el paralelismo en la lectura de datos de entrada. Por ejemplo, si tiene m\u00e1s n\u00facleos, puede aumentar la cantidad de tareas paralelas, lo que garantizar\u00e1 el uso de todos los n\u00facleos del cl\u00faster y aumentar\u00e1 la velocidad de la tarea."}, {"cell_type": "markdown", "id": "dbb68d52-b0ce-428e-b076-ee3328c4f722", "metadata": {}, "source": "##### Shuffle\n\nUna de las principales razones por las que la mayor\u00eda de los trabajos se retrasan en el rendimiento es, la mayor\u00eda de las veces, porque las particiones aleatorias no cuentan correctamente. De forma predeterminada, el valor se establece en 200. En casi todas las situaciones, esto no es lo ideal. Si est\u00e1 tratando con una capacidad aleatoria de menos de 20 GB, 200 est\u00e1 bien, pero de lo contrario, debe cambiarse. En la mayor\u00eda de los casos, puede usar la siguiente ecuaci\u00f3n para encontrar el valor correcto:\n>`Recuento de particiones = Datos de entrada de etapa/Tama\u00f1o de destino` donde <br>\n`Etapa aleatoria m\u00e1s grande (Tama\u00f1o de destino) < 200 MB/partici\u00f3n` en la mayor\u00eda de los casos.<br>\nLa propiedad `spark.sql.shuffle.partitions` se utiliza para establecer el valor ideal de recuento de particiones.\n\nSi alguna vez nota ese tama\u00f1o objetivo en el rango de TB, hay algo terriblemente mal y es posible que desee volver a cambiarlo a 200 o recalcularlo. Las particiones aleatorias se pueden configurar para cada acci\u00f3n (no transformaci\u00f3n) en el script de Spark.\n\nUsemos un ejemplo para explicar este escenario: <br>\nSuponga que la entrada de etapa aleatoria = 210 GB. <br>\nRecuento de particiones = Datos de entrada de etapa/Tama\u00f1o de destino = 210 000 MB/200 MB = 1050. <br>\nComo puede ver, mis particiones aleatorias deber\u00edan ser 1050, no 200.\n\nPero, si su cl\u00faster tiene 2000 n\u00facleos, configure sus particiones aleatorias en 2000.\n>En un cl\u00faster grande que se ocupa de un gran trabajo de datos, nunca establezca sus particiones aleatorias en menos que su recuento total de n\u00facleos."}, {"cell_type": "markdown", "id": "50d39cd1-f43a-4bbc-8cb4-830a030d3d35", "metadata": {}, "source": "##### Output"}, {"cell_type": "markdown", "id": "6b4b95af-8cc9-4ecb-80df-fbcf478b527f", "metadata": {}, "source": "Hay diferentes m\u00e9todos para escribir los datos. Puede controlar el tama\u00f1o, la composici\u00f3n, la cantidad de archivos en la salida e incluso la cantidad de registros en cada archivo mientras escribe los datos. Mientras escribe los datos, puede aumentar el paralelismo, asegurando as\u00ed que utiliza todos los recursos que tiene. Pero este enfoque conducir\u00eda a una mayor cantidad de archivos m\u00e1s peque\u00f1os. Por lo general, esto no es un problema, pero si desea archivos m\u00e1s grandes, deber\u00e1 usar una de las t\u00e9cnicas de compactaci\u00f3n, preferiblemente en un cl\u00faster con una configuraci\u00f3n menor. Hay varias formas de cambiar la composici\u00f3n de la salida. Tenga en cuenta estos dos sobre la composici\u00f3n:\n1. Coalesce: Use esto para reducir el n\u00famero de particiones.\n2. Repartici\u00f3n: use esto muy raramente y nunca para reducir la cantidad de particiones<br>\n    una. Separador de rangos: divide los datos en funci\u00f3n de alg\u00fan orden ordenado O un conjunto de rangos ordenados de claves. <br>\n    b. Hash Partitioner: distribuye los datos en la partici\u00f3n en funci\u00f3n del valor de la clave. La partici\u00f3n hash puede sesgar los datos distribuidos."}, {"cell_type": "markdown", "id": "119180e8-8c2e-4b5b-94aa-c982587c9175", "metadata": {}, "source": "<a id='mejores pr\u00e1cticas'></a>\n### Mejores pr\u00e1cticas\n\nTrate de incorporar estos a sus h\u00e1bitos de codificaci\u00f3n para un mejor rendimiento:\n1.   1. No utilice NOT IN, sino NOT EXISTS.\n2.   Elimine los recuentos, los recuentos distintos (utilice approxCountDIstinct).\n3.   Elimine los duplicados antes de tiempo.\n4.   Prefiera siempre las funciones SQL sobre PandasUDF.\n5.   Utilizar eficazmente las particiones. \n6.   Intentar que la utilizaci\u00f3n del cl\u00faster sea al menos del 70%.\n\n"}, {"cell_type": "markdown", "id": "b404d5cc-7100-4793-aa9b-202ace1792c5", "metadata": {}, "source": "![Spark Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png)\n# Qu\u00e9 es Apache Spark ?\n\nApache Spark es un motor de c\u00e1lculo \"unificado\" y un conjunto de \"bibliotecas\" para el \"procesamiento paralelo de datos\" en clusters de ordenadores.\n\nEn lo que respecta a la velocidad, Spark \"ampl\u00eda\" el popular modelo \"MapReduce\" para soportar de forma eficiente m\u00e1s tipos de c\u00e1lculos, incluidas las consultas interactivas y el procesamiento de flujos. Una de las principales caracter\u00edsticas que ofrece Spark en cuanto a velocidad es la capacidad de ejecutar c\u00e1lculos en memoria, pero el sistema tambi\u00e9n es m\u00e1s eficiente que MapReduce para aplicaciones complejas que se ejecutan en disco.\n\n<center><img src=\"https://i.ibb.co/4tKqXdm/Spark.png\" alt= \"Spark\"></img>\n<br>\n<em>Simple illustration of all that Spark has to offer an end user<em></center>\n    \n\n"}, {"cell_type": "markdown", "id": "4c7d7e5a-7698-4d07-aff9-b3777a70bfa1", "metadata": {}, "source": "## La filosof\u00eda de Apache Spark\n*Desglosemos nuestra descripci\u00f3n de Apache Spark -un motor de computaci\u00f3n unificado y un conjunto de bibliotecas para big data- en sus componentes clave.*\n\n## Una pila unificada\nEl proyecto Spark contiene m\u00faltiples componentes estrechamente integrados. \n- En su n\u00facleo, Spark es un \"motor computacional\" que se encarga de programar, distribuir y supervisar las aplicaciones\n- Impulsa m\u00faltiples componentes de nivel superior especializados en diversas cargas de trabajo, como SQL o el aprendizaje autom\u00e1tico\n- Todas las bibliotecas y componentes de nivel superior de la pila se benefician de las mejoras en las capas inferiores.\n    > Por ejemplo, cuando el motor central de Spark a\u00f1ade una optimizaci\u00f3n, las bibliotecas de SQL y de aprendizaje autom\u00e1tico tambi\u00e9n se aceleran. \n- Los costes asociados a la ejecuci\u00f3n de la pila se reducen al m\u00ednimo, ya que en lugar de ejecutar entre 5 y 10 procesos independientes, una organizaci\u00f3n s\u00f3lo necesita ejecutar uno.\n- Una de las mayores ventajas de la integraci\u00f3n estrecha es la capacidad de construir aplicaciones que combinan sin problemas diferentes modelos de procesamiento.\n    > Por ejemplo, en Spark se puede escribir una aplicaci\u00f3n que utilice el aprendizaje autom\u00e1tico para clasificar los datos en tiempo real a medida que se reciben de fuentes de flujo. Simult\u00e1neamente, los analistas pueden consultar los datos resultantes, tambi\u00e9n en tiempo real, a trav\u00e9s de SQL"}, {"cell_type": "markdown", "id": "5d8a556f-8ab1-4e11-bdca-3b93b7ffccc9", "metadata": {}, "source": "## Computing engine\n\nAl mismo tiempo que Spark se esfuerza por la unificaci\u00f3n, limita cuidadosamente su alcance a un motor de computaci\u00f3n. \n- Podemos utilizar Spark con una amplia variedad de sistemas de almacenamiento persistente, incluyendo sistemas de almacenamiento en la nube como Azure Storage, Amazon S3 y Dataproc, sistemas de archivos distribuidos como Apache Hadoop, almacenes de valores clave como Apache Cassandra y buses de mensajes como Apache Kafka. \n- Spark no almacena los datos a largo plazo por s\u00ed mismo, ni favorece uno sobre otro.\n- Los datos son computacionalmente caros de mover, por lo que Spark se centra en realizar c\u00e1lculos sobre los datos, sin importar d\u00f3nde residan."}, {"cell_type": "markdown", "id": "9da2a67c-4eb2-48d6-b2e9-be3951623b73", "metadata": {}, "source": "## Libraries\n\nEl \u00faltimo componente de Spark son sus librer\u00edas, que se basan en su dise\u00f1o como motor unificado para proporcionar una API unificada para las tareas comunes de an\u00e1lisis de datos. Las librer\u00edas est\u00e1ndar de Spark son en realidad el grueso de los proyectos de c\u00f3digo abierto\n- Spark incluye librer\u00edas para SQL y datos estructurados (Spark SQL), aprendizaje autom\u00e1tico (MLlib), procesamiento de flujos (Spark Streaming y la m\u00e1s reciente Structured Streaming) y an\u00e1lisis de gr\u00e1ficos (GraphX). \n- Adem\u00e1s de estas librer\u00edas, hay cientos de librer\u00edas externas de c\u00f3digo abierto que van desde conectores para varios sistemas de almacenamiento hasta algoritmos de aprendizaje autom\u00e1tico."}, {"cell_type": "markdown", "id": "9e4613ab-946b-4cb7-82cf-c7c37f65efbd", "metadata": {}, "source": "## Arquitectura b\u00e1sica de Spark\n\nLas m\u00e1quinas solas no tienen suficiente potencia y recursos para realizar c\u00e1lculos sobre grandes cantidades de informaci\u00f3n. Un `cluster`, o grupo, de ordenadores, agrupa los recursos de muchas m\u00e1quinas juntas, d\u00e1ndonos la capacidad de utilizar todos los recursos acumulados como si fueran un solo ordenador. ero, un grupo de m\u00e1quinas por s\u00ed solo no es potente, se necesita un marco para coordinar el trabajo entre ellas. Spark hace precisamente eso, gestionar y coordinar la ejecuci\u00f3n de\ntareas sobre los datos en un cl\u00faster de ordenadores.\n\nEl cl\u00faster de m\u00e1quinas que Spark utilizar\u00e1 para ejecutar las tareas es gestionado por un gestor de cl\u00fasteres como\n`Spark's standalone cluster manager`, `YARN`, o `Mesos`. [YARN](https://es.wikipedia.org/wiki/Yarn_(Facebook))"}, {"cell_type": "markdown", "id": "bb8e2e50-d530-47f9-940b-1d1d3a795b00", "metadata": {}, "source": "## Spark Applications\n\n**Las aplicaciones Spark consisten en un proceso `driver` y un conjunto de procesos `ejecutores`.\n\n- El proceso `driver` es el coraz\u00f3n de una aplicaci\u00f3n Spark, su funci\u00f3n main(), se sit\u00faa en un nodo del cluster, y es responsable de tres cosas\n    - mantener la informaci\u00f3n sobre la aplicaci\u00f3n Spark\n    - responder a un programa o entrada del usuario\n    - Analizar, distribuir y programar el trabajo entre los ejecutores\n- Los ejecutores son responsables de llevar a cabo el trabajo que el controlador les asigna. Esto significa que cada ejecutor es responsable de s\u00f3lo dos cosas\n    - ejecutar el c\u00f3digo asignado por el controlador,\n    - informar del estado del c\u00e1lculo en ese ejecutor al nodo controlador\n    \n    <center><img src=\"https://i.ibb.co/BBKv55G/Spark-Application.png\" alt=\"Spark-Application\" border=\"0\"><br>En esta ilustraci\u00f3n vemos a la izquierda nuestro controlador y a la derecha los cuatro ejecutores de la derecha. En este diagrama se elimina el concepto de nodos del cluster. El usuario puede especificar cu\u00e1ntos ejecutores deben caer en cada nodo a trav\u00e9s de las configuraciones.\n    <em></em></center>\n    <br>\n    \n     **_NOTA:_** Spark, adem\u00e1s de su modo cluster, tambi\u00e9n tiene un modo local. El controlador y los ejecutores son simplemente procesos, lo que significa que pueden vivir en la misma m\u00e1quina o en m\u00e1quinas diferentes. En modo local, el controlador y los ejecutores se ejecutan (como hilos) en su ordenador individual en lugar de un cl\u00faster."}, {"cell_type": "markdown", "id": "5a931c85-33d8-4efb-9656-a0cf119c6807", "metadata": {}, "source": "## APIs de lenguaje de Spark"}, {"cell_type": "markdown", "id": "66b7e2c5-62dc-4fe5-92a8-de9abdaba8e5", "metadata": {}, "source": "Las APIs de lenguaje de Spark permiten ejecutar el c\u00f3digo de Spark utilizando varios lenguajes de programaci\u00f3n.\n\n- Scala\n- Java\n- Python\n- SQL\n- R\n\nHay un objeto `SparkSession` disponible para el usuario, que es el punto de entrada para ejecutar el c\u00f3digo de Spark. Cuando se utiliza Spark desde Python o R, no se escriben instrucciones expl\u00edcitas en la JVM; en su lugar, se escribe c\u00f3digo Python y R que Spark traduce en c\u00f3digo que luego puede ejecutar en las JVMs ejecutoras.\n\n<center><img src=\"https://i.ibb.co/8B1kjnq/Language-API.png\" alt=\"Language-API\" border=\"0\">\n<em><br>The relationship between the SparkSession and Spark\u2019s Language API</em></center>\n<br>\nSpark tiene dos conjuntos fundamentales de APIs: las APIs de bajo nivel `no estructuradas`, y las APIs de alto nivel `estructuradas`.\n<center><img src=\"https://i.ibb.co/PW8KCG2/Spark-APIs.png\" alt=\"Spark-APIs\" border=\"0\">"}, {"cell_type": "markdown", "id": "fb7c01fc-9a76-40f5-8360-68c075609cb5", "metadata": {}, "source": "# APIs de bajo nivel no estructuradas\n\nEn este cuaderno discutiremos el concepto fundamental m\u00e1s antiguo de spark llamado *RDDs(Resilient distributed\ndatasets)*.\n\n<br> \nPara entender realmente c\u00f3mo funciona Spark, `hay que entender la esencia de los RDDs`.Estos proporcionan una base extremadamente s\u00f3lida sobre la que se construyen otras abstracciones. A partir de Spark 2.0, los usuarios de Spark tendr\u00e1n menos necesidades de interactuar directamente con los RDD, pero es esencial tener un modelo mental s\u00f3lido de c\u00f3mo funcionan los RDD. `En pocas palabras, Spark gira en torno al concepto de RDD`.\n\n## Re-Introduction to RDDs\n\nUn RDD en Spark es simplemente una colecci\u00f3n distribuida inmutable de objetos. Cada uno se divide en m\u00faltiples particiones, que pueden ser calculadas en diferentes nodos del cl\u00faster.\n<br>\nLos RDDs son `inmutables`, `tolerantes a fallos`, `estructuras de datos paralelas` que permiten a los usuarios persistir expl\u00edcitamente los resultados intermedios `en memoria`, controlar su partici\u00f3n para optimizar la colocaci\u00f3n de los datos, y `manipularlos` utilizando un rico conjunto de `operadores`.\n\n\n## Inmutable\n\nLos RDDs est\u00e1n dise\u00f1ados para ser inmutables, lo que significa que usted `no puede` modificar espec\u00edficamente una fila particular en el conjunto de datos representado por ese RDD. Puedes llamar a una de las operaciones disponibles para manipular las filas del RDD de la forma que quieras, pero esa operaci\u00f3n `devolver\u00e1 un nuevo RDD`. El `RDD b\u00e1sico permanecer\u00e1 sin cambios`, y el nuevo RDD\ncontendr\u00e1 los datos de la forma que usted desee. *Spark aprovecha la Inmutabilidad para proporcionar eficientemente la capacidad de tolerancia a fallos.* \n\n\n\n## Tolerancia a los fallos\n\nLa capacidad de procesar m\u00faltiples conjuntos de datos en paralelo suele requerir un cl\u00faster de m\u00e1quinas para alojar y ejecutar la l\u00f3gica computacional. Si una o m\u00e1s m\u00e1quinas mueren debido a circunstancias inesperadas, \u00bfqu\u00e9 pasa con los datos en esas m\u00e1quinas?  Spark se encarga autom\u00e1ticamente de gestionar el fallo en nombre de sus usuarios reconstruyendo la parte que ha fallado utilizando la informaci\u00f3n del linaje.\n\n## Estructuras de Datos Paralelas\n\nSuponga que tiene una gran cantidad de datos y necesita procesar todas y cada una de las filas del conjunto de datos. Una soluci\u00f3n ser\u00eda iterar sobre cada fila y procesarla una por una. Pero eso ser\u00eda muy lento. As\u00ed que, en lugar de eso, dividiremos la enorme cantidad de datos en trozos m\u00e1s peque\u00f1os. Cada trozo contiene una colecci\u00f3n de filas, y todos los trozos se procesan en paralelo. De ah\u00ed viene la expresi\u00f3n \"estructuras de datos paralelas\".\n\n\n## Computaci\u00f3n en memoria\n\nLa idea de acelerar el c\u00e1lculo de grandes conjuntos de datos que residen en discos de forma paralela utilizando un cl\u00faster de m\u00e1quinas fue introducida por un documento de MapReduce de Google. RDD ampl\u00eda los l\u00edmites de la velocidad introduciendo una idea novedosa, que es la capacidad de realizar c\u00e1lculos distribuidos en memoria.\n\n## Operaciones con RDDs\n\nLos RDDs proporcionan un rico conjunto de operaciones de procesamiento de datos com\u00fanmente necesarias. Incluyen la capacidad de realizar transformaciones de datos, filtrado, agrupaci\u00f3n, uni\u00f3n, agregaci\u00f3n, ordenaci\u00f3n y recuento.\n\nCada fila de un conjunto de datos se representa como un objeto Java, y la estructura de este objeto Java es opaca para Spark. El usuario de RDD tiene un control total sobre c\u00f3mo manipular este objeto Java. Esta flexibilidad viene con muchas responsabilidades, lo que significa que algunas de las operaciones com\u00fanmente necesarias, como la media de c\u00e1lculo, tendr\u00e1n que ser hechas a mano. Las abstracciones de nivel superior, como el componente SQL de Spark, proporcionar\u00e1n esta funcionalidad fuera de la caja.\n\n***Las operaciones del RDD se clasifican en dos tipos: `transformaciones` y `acciones`***\n\n| Tipo de operaci\u00f3n: Evaluaci\u00f3n: Valor devuelto.\n|--|--|--|\n| Transformaci\u00f3n: perezoso, otro RDD.\n| Acci\u00f3n: Ansioso: Alg\u00fan resultado o escribir el resultado en el disco.\n\nLas operaciones de transformaci\u00f3n se eval\u00faan de forma perezosa, lo que significa que Spark retrasar\u00e1 las evaluaciones de las operaciones invocadas hasta que se realice una acci\u00f3n. En otras palabras, las operaciones de transformaci\u00f3n se limitan a registrar la l\u00f3gica de transformaci\u00f3n especificada y la aplicar\u00e1n en un momento posterior. Por otro lado, la invocaci\u00f3n de una operaci\u00f3n de acci\u00f3n desencadenar\u00e1 la evaluaci\u00f3n de todas las transformaciones que la precedieron, y devolver\u00e1 alg\u00fan resultado al controlador o escribir\u00e1 datos en un sistema de almacenamiento, como HDFS o el sistema de archivos local."}, {"cell_type": "markdown", "id": "f0af4161-0483-42d4-9ee3-00d313bf4f6d", "metadata": {}, "source": "## Initialising Spark"}, {"cell_type": "code", "execution_count": 1, "id": "8d24d6e8-1d47-4845-90c5-6ce4bbf423c8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.3.2)\nRequirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.10/site-packages (1.4.4)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (1.22.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip install pyspark pandas"}, {"cell_type": "code", "execution_count": 2, "id": "ecfa7cef-c612-4341-83fe-89464f42365c", "metadata": {}, "outputs": [], "source": "from pyspark import SparkConf, SparkContext"}, {"cell_type": "code", "execution_count": 3, "id": "e4de390d-7fbb-4d01-90a4-6456d04a3ede", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/05/27 20:57:20 INFO SparkEnv: Registering MapOutputTracker\n25/05/27 20:57:20 INFO SparkEnv: Registering BlockManagerMaster\n25/05/27 20:57:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/05/27 20:57:20 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\").set(\"spark.files.overwrite\", \"true\")\nsc = SparkContext(conf = conf)"}, {"cell_type": "markdown", "id": "7ff55bba-3a2b-4eff-8eb9-b65806fec596", "metadata": {}, "source": "## Creaci\u00f3n de RDDs"}, {"cell_type": "markdown", "id": "c289c60f-6050-4200-add8-5280b7f661cb", "metadata": {}, "source": "**Hay dos maneras de crear RDDs:**\n\n**La primera forma de crear un RDD es paralelizando un objeto de Python, es decir, convirti\u00e9ndolo en un conjunto de datos distribuido que puede ser operado en paralelo."}, {"cell_type": "code", "execution_count": 4, "id": "2ea5666d-6b11-46cf-8354-4593c3d8531e", "metadata": {}, "outputs": [], "source": "stringList = [\"Spark is awesome\",\"Spark is cool\"]\nstringRDD = sc.parallelize(stringList)"}, {"cell_type": "code", "execution_count": 5, "id": "b824327e-e017-4bb2-9c75-f31ddc923f7a", "metadata": {}, "outputs": [{"data": {"text/plain": "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "stringRDD"}, {"cell_type": "code", "execution_count": 6, "id": "4f727eb3-8f29-4f3e-a153-4ed860468d33", "metadata": {}, "outputs": [{"data": {"text/plain": "['Spark is awesome', 'Spark is cool']"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "stringRDD.collect()"}, {"cell_type": "markdown", "id": "eb9a3016-3218-4044-b169-26b3968c7130", "metadata": {}, "source": "**La segunda forma de crear un RDD es leer un conjunto de datos de un sistema de almacenamiento, que puede ser un sistema de archivos del ordenador local, HDFS, Cassandra, Amazon S3, etc."}, {"cell_type": "code", "execution_count": 7, "id": "11216d72-b429-4cea-85cc-1b88b5da8d11", "metadata": {}, "outputs": [], "source": "from pyspark import SparkFiles\n\ndata_file_https_url = \"https://raw.githubusercontent.com/databricks/spark-training/refs/heads/master/data/movielens/medium/ratings.dat\"\nsc.addFile(data_file_https_url)\nfilePath  = 'file://' +SparkFiles.get('ratings.dat')\n\nratings = sc.textFile(filePath) \nratings = ratings.map(lambda line: line.split('::')) "}, {"cell_type": "code", "execution_count": 8, "id": "1d4df33a-97ea-41ff-a883-70230c04f724", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "['1', '1193', '5', '978300760']\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "print(ratings.first())"}, {"cell_type": "code", "execution_count": 9, "id": "35fa8b2b-596b-4d63-b682-ef621ee1ccc0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[['1', '1193', '5', '978300760'],\n ['1', '661', '3', '978302109'],\n ['1', '914', '3', '978301968'],\n ['1', '3408', '4', '978300275'],\n ['1', '2355', '5', '978824291']]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "ratings.take(5)"}, {"cell_type": "code", "execution_count": 10, "id": "d5cdff47-7ad7-4f4f-b585-d20659a92420", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "1000209"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "ratings.count()"}, {"cell_type": "markdown", "id": "53bd53b5-b34d-4281-b8ef-fc52e9a367d1", "metadata": {}, "source": "En este ejemplo en particular ten\u00edamos 1M de filas llamando a .collect() no tom\u00f3 mucho tiempo pero si tu RDD contiene 100 mil millones de filas, entonces no es una buena idea invocar la acci\u00f3n collect porque el programa del driver muy probablemente no tiene suficiente memoria para mantener todas esas filas. Como resultado, el controlador probablemente se encontrar\u00e1 con un error de falta de memoria y su aplicaci\u00f3n Spark o shell morir\u00e1. Esta acci\u00f3n se utiliza normalmente una vez que el RDD se ha filtrado a un tama\u00f1o m\u00e1s peque\u00f1o que puede encajar en el tama\u00f1o de la memoria del programa controlador. "}, {"cell_type": "markdown", "id": "6ed7eef0-4e0c-4e3a-81b9-ad7b3140a3ac", "metadata": {}, "source": "## Transformaciones\n\nLas transformaciones son operaciones sobre RDDs que devuelven un nuevo RDD. Los RDDs transformados se calculan de forma perezosa, s\u00f3lo cuando se\nse utilizan en una acci\u00f3n.\n\nLa siguiente tabla describe las transformaciones m\u00e1s utilizadas.\n\n\n<table>\n<tbody><tr><th style=\"width:25%\">Transformation</th><th>Meaning</th></tr>\n<tr>\n  <td> <b>map</b>(<i>func</i>) </td>\n  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n</tr>\n<tr>\n  <td> <b>filter</b>(<i>func</i>) </td>\n  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n</tr>\n<tr>\n  <td> <b>flatMap</b>(<i>func</i>) </td>\n  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>\n</tr>\n<tr>\n  <td> <b>mapPartitions</b>(<i>func</i>) <a name=\"MapPartLink\"></a> </td>\n  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type\n    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. </td>\n</tr>\n<tr>\n  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>\n  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of\n  the partition, so <i>func</i> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.\n  </td>\n</tr>\n<tr>\n  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n</tr>\n<tr>\n  <td> <b>union</b>(<i>otherDataset</i>) </td>\n  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>\n</tr>\n<tr>\n  <td> <b>intersection</b>(<i>otherDataset</i>) </td>\n  <td> Return a new RDD that contains the intersection of elements in the source dataset and the argument. </td>\n</tr>\n<tr>\n  <td> <b>distinct</b>([<i>numPartitions</i>])) </td>\n  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>\n</tr>\n<tr>\n  <td> <b>groupByKey</b>([<i>numPartitions</i>]) <a name=\"GroupByLink\"></a> </td>\n  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>\n    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or\n      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better\n      performance.\n    <br>\n    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n      You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.\n  </td>\n</tr>\n<tr>\n  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numPartitions</i>]) <a name=\"ReduceByLink\"></a> </td>\n  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n</tr>\n<tr>\n  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numPartitions</i>]) <a name=\"AggregateByLink\"></a> </td>\n  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n</tr>\n<tr>\n  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numPartitions</i>]) <a name=\"SortByLink\"></a> </td>\n  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n</tr>\n<tr>\n  <td> <b>join</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name=\"JoinLink\"></a> </td>\n  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n  </td>\n</tr>\n<tr>\n  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name=\"CogroupLink\"></a> </td>\n  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>. </td>\n</tr>\n<tr>\n  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>\n  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>\n</tr>\n<tr>\n  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n  <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>\n</tr>\n<tr>\n  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name=\"CoalesceLink\"></a> </td>\n  <td> Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently\n    after filtering down a large dataset. </td>\n</tr>\n<tr>\n  <td> <b>repartition</b>(<i>numPartitions</i>) </td>\n  <td> Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.\n    This always shuffles all data over the network. <a name=\"RepartitionLink\"></a></td>\n</tr>\n<tr>\n  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name=\"Repartition2Link\"></a></td>\n  <td> Repartition the RDD according to the given partitioner and, within each resulting partition,\n  sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within\n  each partition because it can push the sorting down into the shuffle machinery. </td>\n</tr>\n</tbody></table>"}, {"cell_type": "markdown", "id": "41568413-5513-4574-8073-a87a686280da", "metadata": {}, "source": "## Ejemplos de transformaci\u00f3n"}, {"cell_type": "markdown", "id": "cec847bd-a68f-4394-879f-786cbc9359ee", "metadata": {}, "source": "### Transformaci\u00f3n del mapa\n\n*Devuelve un nuevo RDD aplicando una funci\u00f3n a cada elemento de este RDD*"}, {"cell_type": "code", "execution_count": 11, "id": "45072394-4c82-4063-a42b-b50146b5e85a", "metadata": {}, "outputs": [{"data": {"text/plain": "['SPARK IS AWESOME', 'SPARK IS COOL']"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "stringRDD_uppercase= stringRDD.map(lambda x: x.upper())\nstringRDD_uppercase.collect()"}, {"cell_type": "code", "execution_count": 12, "id": "96c1e23a-830f-4fad-b718-2fa1e2d8110c", "metadata": {}, "outputs": [{"data": {"text/plain": "['SpArK Is aWeSoMe', 'SpArK Is cOoL']"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "def alternate_char_upper(text):\n    new_text= []\n    for i, character in enumerate(text):\n        if i % 2 == 0:\n            new_text.append(character.upper())\n        else:\n            new_text.append(character)\n    return ''.join(new_text)\nstringRDD_alternate_uppercase= stringRDD.map(alternate_char_upper)\nstringRDD_alternate_uppercase.collect()"}, {"cell_type": "markdown", "id": "94191fa1-1b17-4b35-b805-6591f4eab453", "metadata": {}, "source": "### Flat Map Transfermation\n\n*Devuelve un nuevo RDD aplicando primero una funci\u00f3n a todos los elementos de este RDD, y luego aplanando los resultados*."}, {"cell_type": "code", "execution_count": 13, "id": "09d7e6cd-5418-4d4f-a70a-f6f14eb7e1c5", "metadata": {}, "outputs": [{"data": {"text/plain": "['Spark', 'is', 'awesome', 'Spark', 'is', 'cool']"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "flatMap_Split= stringRDD.flatMap(lambda x: x.split(\" \"))\nflatMap_Split.collect()"}, {"cell_type": "markdown", "id": "55e2fc53-d45f-49fd-af2e-556b801e1f7f", "metadata": {}, "source": "### Diferencia entre Map y FlatMap "}, {"cell_type": "code", "execution_count": 14, "id": "72e93c46-784e-49b9-8a6b-a0c2110c03d2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Split using Map transformation:\n"}, {"data": {"text/plain": "[['Spark', 'is', 'awesome'], ['Spark', 'is', 'cool']]"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "print(\"Split using Map transformation:\")\nmap_Split= stringRDD.map(lambda x: x.split(\" \"))\nmap_Split.collect()"}, {"cell_type": "code", "execution_count": 15, "id": "045dbc03-a301-475b-9cd0-9b4d660221d8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Split using FlatMap transformation:\n"}, {"data": {"text/plain": "['Spark', 'is', 'awesome', 'Spark', 'is', 'cool']"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "print(\"Split using FlatMap transformation:\")\nflatMap_Split.collect()"}, {"cell_type": "markdown", "id": "edaa11c8-5ea1-49a8-b591-63f524db7f84", "metadata": {}, "source": "## Transformaci\u00f3n MapPartitions\n\n*Devuelve un nuevo RDD aplicando una funci\u00f3n a cada partici\u00f3n de este RDD*"}, {"cell_type": "code", "execution_count": 16, "id": "a848d1fe-9323-478b-b8c0-619cd9ebce21", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[[1], [2, 3]]\n[[1, 42], [5, 42]]\n"}], "source": "x = sc.parallelize([1,2,3], 2)\ndef f(iterator): yield sum(iterator); yield 42\ny = x.mapPartitions(f)\n# glom() flattens elements on the same partition\nprint(x.glom().collect())\nprint(y.glom().collect())"}, {"cell_type": "markdown", "id": "146b3790-614f-40e3-a27a-c6aad04f5e1e", "metadata": {}, "source": "### Coalesce Transformation\n\n*Devuelve un nuevo RDD reducido a un n\u00famero menor de particiones*.\n\n`coalesce(numPartitions, shuffle=False)`"}, {"cell_type": "code", "execution_count": 17, "id": "e02a8851-d33b-48c6-ab29-cdc075ee3992", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[[1], [2, 3], [4, 5]]\n[[1], [2, 3, 4, 5]]\n"}], "source": "x = sc.parallelize([1, 2, 3, 4, 5], 3)\ny = x.coalesce(2)\nprint(x.glom().collect())\nprint(y.glom().collect())"}, {"cell_type": "markdown", "id": "4424329c-4f52-485e-9c2d-a11fc4bb1877", "metadata": {}, "source": "### KeyBy Transformation\n\n*Crea un RDD de pares, formando un par por cada elemento del RDD original. La clave del par se calcula a partir del valor mediante una funci\u00f3n suministrada por el usuario."}, {"cell_type": "code", "execution_count": 18, "id": "1aaf6ac8-9486-4830-8a40-72c8d6a3bdbd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('J', 'John'), ('F', 'Fred'), ('A', 'Anna'), ('J', 'James')]\n"}], "source": "x = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\ny = x.keyBy(lambda w: w[0])\nprint(y.collect())"}, {"cell_type": "markdown", "id": "50d496e7-1935-4753-8440-eb13fd109978", "metadata": {}, "source": "### Transformaci\u00f3n PartitionBy\n\n*Devuelve un nuevo RDD con el n\u00famero de particiones especificado, colocando los elementos originales en la partici\u00f3n devuelta por una funci\u00f3n suministrada por el usuario*\n\n`partitionBy(numPartitions, partitioner=portable_hash)`"}, {"cell_type": "code", "execution_count": 19, "id": "dac6b8ac-8f87-4a5a-88d8-d8bdf2980a49", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[[('J', 'James')], [('F', 'Fred')], [('A', 'Anna'), ('J', 'John')]]\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 15:======================================>                   (2 + 1) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "[[('F', 'Fred'), ('A', 'Anna')], [('J', 'James'), ('J', 'John')]]\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "x = sc.parallelize([('J','James'),\n                    ('F','Fred'),\n                    ('A','Anna'),\n                    ('J','John')], 3)\n\ny = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\nprint(x.glom().collect())\nprint(y.glom().collect())"}, {"cell_type": "markdown", "id": "30e3dc06-71d7-43eb-bf8c-7109aa8e15ec", "metadata": {}, "source": "### Transformaci\u00f3n de la cremallera\n\n*Devuelve un nuevo RDD que contiene pares cuya clave es el elemento del RDD original, y cuyo\nvalor es el elemento correspondiente de ese elemento (misma partici\u00f3n, mismo \u00edndice) en un segundo RDD*.\n\n`zip(otroRDD)`"}, {"cell_type": "code", "execution_count": 20, "id": "bf5f1421-8ebf-4e90-a014-00843f44d484", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[(1, 1), (2, 4), (3, 9)]\n"}], "source": "x = sc.parallelize([1, 2, 3])\ny = x.map(lambda n:n*n)\nz = x.zip(y)\nprint(z.collect())"}, {"cell_type": "markdown", "id": "0670e486-db0f-4724-84fb-c8804380dca0", "metadata": {}, "source": "## Actions\n\n<table class=\"table\">\n<tbody><tr><th>Action</th><th>Meaning</th></tr>\n<tr>\n  <td> <b>reduce</b>(<i>func</i>) </td>\n  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n</tr>\n<tr>\n  <td> <b>collect</b>() </td>\n  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n</tr>\n<tr>\n  <td> <b>count</b>() </td>\n  <td> Return the number of elements in the dataset. </td>\n</tr>\n<tr>\n  <td> <b>first</b>() </td>\n  <td> Return the first element of the dataset (similar to take(1)). </td>\n</tr>\n<tr>\n  <td> <b>take</b>(<i>n</i>) </td>\n  <td> Return an array with the first <i>n</i> elements of the dataset. </td>\n</tr>\n<tr>\n  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>\n</tr>\n<tr>\n  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n  <td> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n</tr>\n<tr>\n  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>\n  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n</tr>\n<tr>\n  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br> (Java and Scala) </td>\n  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also\n   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>\n</tr>\n<tr>\n  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br> (Java and Scala) </td>\n  <td> Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using\n    <code>SparkContext.objectFile()</code>. </td>\n</tr>\n<tr>\n  <td> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n  <td> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>\n</tr>\n<tr>\n  <td> <b>foreach</b>(<i>func</i>) </td>\n  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.\n  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See Understanding closures for more details.</td>\n</tr>\n</tbody></table>"}, {"cell_type": "markdown", "id": "31e61156-54b8-4baf-9e10-1454e23b8cc1", "metadata": {}, "source": "### GetNumpartitions Action\n\n*Retorna el n\u00famero de particiones en RDD*"}, {"cell_type": "code", "execution_count": 21, "id": "450e7ec7-1c61-40a2-b3ea-5e7b673c50ec", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[[1], [2, 3]]\n2\n"}], "source": "x = sc.parallelize([1,2,3], 2)\ny = x.getNumPartitions()\nprint(x.glom().collect())\nprint(y)"}, {"cell_type": "code", "execution_count": 22, "id": "ce73c62e-5377-4a11-964a-a5eb1aab7fef", "metadata": {}, "outputs": [], "source": "sc.stop()"}, {"cell_type": "code", "execution_count": null, "id": "37ee972f-3937-4002-bb5f-73538c74dba4", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "54baa1cf-0ea8-4960-998e-7f0fa476ac36", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}