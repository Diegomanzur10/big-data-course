{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd2ec09-f6a9-4e22-913d-705418eba7de",
   "metadata": {},
   "source": [
    "<a id='big-data-pyspark-y-colaboratorio'></a>\n",
    "## Big data, PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d5e11-057b-4818-97aa-12ff369a4d06",
   "metadata": {},
   "source": [
    "Big data generalmente significa datos de un volumen tan grande que las soluciones normales de almacenamiento de datos no pueden almacenarlos y procesarlos de manera eficiente. En esta era, los datos se generan a un ritmo absurdo. Se recopilan datos por cada movimiento que realiza una persona. La mayor parte de los grandes datos proviene de tres fuentes principales:\n",
    "<ol>\n",
    "   <li>Datos sociales</li>\n",
    "   <li>Datos de la máquina</li>\n",
    "   <li>Datos transaccionales</li>\n",
    "</ol>\n",
    "\n",
    "Algunos ejemplos comunes de las fuentes de dichos datos incluyen búsquedas en Internet, publicaciones en Facebook, cámaras de timbre, relojes inteligentes, historial de compras en línea, etc. Cada acción crea datos, es solo una cuestión de si hay una manera de recopilarlos o no. Pero lo interesante es que de todos estos datos recopilados, ni siquiera el 5% se está utilizando por completo. Existe una gran demanda de profesionales de big data en la industria. Aunque el número de graduados con especialización en big data está aumentando, el problema es que no tienen el conocimiento práctico sobre escenarios de big data, lo que conduce a malas arquitecturas y métodos ineficientes de procesamiento de datos.\n",
    "\n",
    ">Si está interesado en saber más sobre el panorama y las tecnologías involucradas, aquí hay [un artículo](https://hostingtribunal.com/blog/big-data-stats/) ¡que encontré realmente interesante!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba18aa-7a26-4a5a-844b-db43e4e10e96",
   "metadata": {},
   "source": [
    "<a id='pyspark'></a>\n",
    "### PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4addd06-89d0-4a30-8166-d80ff8e353f1",
   "metadata": {},
   "source": [
    "Si está trabajando en el campo de los grandes datos, definitivamente debe haber oído hablar de Spark. Si observa el sitio web de [Apache Spark](https://spark.apache.org/), verá que se dice que es un \"motor de análisis unificado ultrarrápido\". PySpark es una versión de Spark que se utiliza para procesar y analizar volúmenes masivos de datos. Si está familiarizado con Python y lo ha probado con grandes conjuntos de datos, debe saber que el tiempo de ejecución puede ser ridículo. ¡Entra en PySpark!\n",
    "\n",
    "Imagine que sus datos residen de manera distribuida en diferentes lugares. Si intenta llevar sus datos a un punto y ejecutar su código allí, no solo sería ineficiente, sino que también causaría problemas de memoria. Ahora digamos que su código va a los datos en lugar de que los datos lleguen a donde está su código. Esto ayudará a evitar el movimiento de datos innecesario que, por lo tanto, reducirá el tiempo de ejecución.\n",
    "\n",
    "PySpark es la API Python de Spark; lo que significa que puede hacer casi todas las cosas que Python puede hacer. Canalizaciones de aprendizaje automático (ML), análisis de datos exploratorios (a escala), ETL para plataforma de datos, ¡y mucho más! Y todos ellos de forma distribuida. Una de las mejores partes de pyspark es que si ya está familiarizado con python, es realmente fácil de aprender.\n",
    "\n",
    "Además de PySpark, existe otro lenguaje llamado Scala que se utiliza para el procesamiento de big data. Scala suele ser 10 veces más rápido que *Python*, ya que es nativo de Hadoop y se basa en JVM. Pero PySpark se está adoptando a un ritmo acelerado debido a la facilidad de uso, la curva de aprendizaje más fácil y las capacidades de ML.\n",
    "\n",
    "Explicaré brevemente cómo funciona un trabajo de PySpark, pero le recomiendo que lea más sobre la [arquitectura](https://data-flair.training/blogs/how-apache-spark-works/) y cómo funciona todo. Ahora, antes de entrar en materia, permítanme hablar primero sobre algunas <u>jergas básicas</u>:\n",
    "\n",
    "<b>Cluster</b> es un conjunto de computadoras conectadas de forma flexible o estrecha que funcionan juntas para que puedan verse como un solo sistema.\n",
    "\n",
    "<b>Hadoop</b> es un marco de código abierto, escalable y tolerante a fallas escrito en Java. Procesa de manera eficiente grandes volúmenes de datos en un grupo de hardware básico. Hadoop no es solo un sistema de almacenamiento, sino que es una plataforma para el almacenamiento y el procesamiento de grandes datos.\n",
    "\n",
    "<b>HDFS</b> (sistema de archivos distribuido Hadoop). Es uno de los sistemas de almacenamiento más fiables del mundo. HDFS es un sistema de archivos de Hadoop diseñado para almacenar archivos muy grandes que se ejecutan en un clúster de hardware básico.\n",
    "\n",
    "<b>MapReduce</b> es un marco de procesamiento de datos, que tiene 2 fases: Mapper y Reducer. El procedimiento map realiza el filtrado y la clasificación, y el método reduce realiza una operación de resumen. Por lo general, se ejecuta en un clúster de Hadoop.\n",
    "\n",
    "<b>Transformación</b> se refiere a las operaciones aplicadas en un conjunto de datos para crear un nuevo conjunto de datos. Filter, groupBy y map son ejemplos de transformaciones.\n",
    "\n",
    "<b>Acciones</b> Las acciones se refieren a una operación que le indica a Spark que realice un cálculo y envíe el resultado al controlador. Este es un ejemplo de acción.\n",
    "\n",
    "¡Bien! Ahora que eso está fuera del camino, permítanme explicar cómo se ejecuta un trabajo de chispa. En terma simple, cada vez que envía un trabajo pyspark, el código se convierte internamente en un programa MapReduce y se ejecuta en la máquina virtual Java. Ahora, uno de los pensamientos que podría estar apareciendo en su mente probablemente sea: <br>`Así que el código se convierte en un programa MapReduce. ¿No significaría eso que MapReduce es más rápido que pySpark?`<br> Bueno, la respuesta es un rotundo NO. Esto es lo que hace que los trabajos de Spark sean especiales. Spark es capaz de manejar una gran cantidad de datos a la vez, en su entorno distribuido. Lo hace a través del <u>procesamiento en memoria</u>, que es lo que lo hace casi 100 veces más rápido que Hadoop. Otro factor que lo hace rápido es <u>Lazy Evaluation</u>. Spark retrasa su evaluación tanto como puede. Cada vez que envía un trabajo, Spark crea un plan de acción sobre cómo ejecutar el código y luego no hace nada. Finalmente, cuando solicita el resultado (es decir, llama a una acción), ejecuta el plan, que es básicamente todas las transformaciones que ha mencionado en su código. Esa es básicamente la esencia de esto.\n",
    "\n",
    "Ahora, por último, quiero hablar sobre más cosas. Spark consta principalmente de 4 módulos:\n",
    "\n",
    "<ol>\n",
    "    <li>Spark SQL: ayuda a escribir programas Spark utilizando consultas similares a SQL.</li>\n",
    "    <li>Spark Streaming: es una extensión de la API principal de Spark que permite el procesamiento de transmisiones de datos en vivo escalable, de alto rendimiento y tolerante a fallas. se usa mucho en el procesamiento de datos de redes sociales.</li>\n",
    "    <li>Spark MLLib: es el componente de aprendizaje automático de SPark. Ayuda a entrenar modelos ML en conjuntos de datos masivos con una eficiencia muy alta. </li>\n",
    "    <li>Spark GraphX: es el componente de visualización de Spark. Permite a los usuarios ver datos como gráficos y como colecciones sin movimiento ni duplicación de datos.</li>\n",
    "</ol>\n",
    "\n",
    "Espero que esta imagen dé una mejor idea de lo que estoy hablando:\n",
    "<img alt=\"Módulos Spark\" src=\"https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2015/11/spark-streaming-datanami.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ee30-0db3-4f19-a44a-0ba4a4837a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark pandas numpy findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117dd322-9b0b-40fc-8822-2455cb6041bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "conf = SparkConf().setAppName('spark-app').setMaster('local[*]')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fb099-d633-40f3-b514-3e94b373f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv'\n",
    "df = spark.createDataFrame(pd.read_csv(url, sep=\";\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbd08b-99fa-40b0-a6f2-55ae70033eee",
   "metadata": {},
   "source": [
    "El comando anterior carga nuestros datos desde un dataframe (DF). Un marco de datos es una estructura de datos etiquetada bidimensional con columnas de tipos potencialmente diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67870200-eebe-449b-b667-772fd9782ff4",
   "metadata": {},
   "source": [
    "<a id='ver-el-marco de datos'></a>\n",
    "### Visualización del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4dfc1-9702-4a6d-839c-ffb2498f2f7c",
   "metadata": {},
   "source": [
    "Hay un par de formas de ver su dataframe (DF) en PySpark:\n",
    "\n",
    "1. `df.take(5)` devolverá una lista de cinco objetos Row.\n",
    "2. `df.collect()` obtendrá todos los datos de todo el DataFrame. Tenga mucho cuidado al usarlo, porque si tiene un gran conjunto de datos, puede bloquear fácilmente el nodo del controlador.\n",
    "3. `df.show()` es el método más utilizado para ver un marco de datos. Hay algunos parámetros que podemos pasar a este método, como el número de filas y el truncamiento. Por ejemplo, `df.show(5, False)` o ` df.show(5, truncate=False)` mostrará los datos completos sin ningún truncamiento.\n",
    "4. `df.limit(5)` **devolverá un nuevo DataFrame** tomando las primeras n filas. Como la chispa se distribuye en la naturaleza, no hay garantía de que `df.limit()` le brinde los mismos resultados cada vez.\n",
    "\n",
    "Veamos algunos de ellos en acción a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ec127-b89d-4804-bd71-a2dafb1e87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ca4ea-dc5d-4b93-901c-69287d3ed449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cdeef6-3519-4d76-8bf4-ec7b30afe504",
   "metadata": {},
   "source": [
    "<a id='ver-columnas-del-marco-de-datos'></a>\n",
    "### Visualización de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2f187-4d07-4e0a-aba6-b961fbb6ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79b59b-5674-4597-8540-e8700272fd4e",
   "metadata": {},
   "source": [
    "<a id='esquema del marco de datos'></a>\n",
    "### Esquema del dataframe\n",
    "\n",
    "Hay dos métodos comúnmente utilizados para ver los tipos de datos de un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22994a-4646-4ccc-a2b9-f674c94cf6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4daf6-f886-4e2f-9698-8bd177c722ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbf679-9103-4797-aa1c-a8341a496c1e",
   "metadata": {},
   "source": [
    "<a id='inferencia-esquema-implícito'></a>\n",
    "#### Esquema de inferencia implícita\n",
    "\n",
    "Podemos usar el parámetro `inferschema=true` para inferir el esquema de entrada automáticamente mientras cargamos los datos. A continuación se muestra un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6371be-02b6-41e5-9b53-41eaeabf88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv'\n",
    "df = spark.createDataFrame(pd.read_csv(url, sep=\";\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fe58e-5aae-48ee-b7e8-1aa821e56def",
   "metadata": {},
   "source": [
    "Como puede ver, el tipo de datos se ha deducido automáticamente incluso con la precisión correcta para el tipo decimal. Un problema que podría surgir aquí es que a veces, cuando tiene que leer varios archivos con diferentes esquemas en diferentes archivos, puede haber un problema con la inferencia implícita que conduce a valores nulos en algunas columnas. Por lo tanto, veamos también cómo definir esquemas explícitamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca8cfe-110f-406a-98dd-839a0023c6b7",
   "metadata": {},
   "source": [
    "<a id='inferencia-esquema-explícito'></a>\n",
    "#### Definición explícita del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44f990-f464-4abd-a44f-f0ca08d05ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a66504-fba0-4d51-a045-ea5c030b7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of the schema in the format column_name, data_type\n",
    "labels = [\n",
    "     ('Car',StringType()),\n",
    "     ('MPG',DoubleType()),\n",
    "     ('Cylinders',IntegerType()),\n",
    "     ('Displacement',DoubleType()),\n",
    "     ('Horsepower',DoubleType()),\n",
    "     ('Weight',DoubleType()),\n",
    "     ('Acceleration',DoubleType()),\n",
    "     ('Model',IntegerType()),\n",
    "     ('Origin',StringType())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad0d9e-a146-47d3-8658-130ba1d9ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the schema that will be passed when reading the csv\n",
    "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f8131-bb71-4c14-be95-01e0239d6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f9082-9617-4114-9c69-9fbb08ba3190",
   "metadata": {},
   "source": [
    "Como podemos ver aquí, los datos se han cargado correctamente con los tipos de datos especificados.\n",
    "\n",
    "<a id='operaciones-del-marco-de-datos-en-columnas'></a>\n",
    "## Operaciones de columnas\n",
    "\n",
    "Veremos lo siguiente en esta sección:\n",
    "\n",
    "1. Selección de columnas\n",
    "2. Selección de varias columnas\n",
    "3. Agregar nuevas columnas\n",
    "4. Cambiar el nombre de las columnas\n",
    "5. Agrupación por columnas\n",
    "6. Eliminación de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab35755-a976-4bb6-a0d1-4688a75731bd",
   "metadata": {},
   "source": [
    "<a id='seleccionar-columnas'></a>\n",
    "### Selección de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834d22d-42c5-4361-939f-a6883b98f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st method\n",
    "# Column name is case sensitive in this usage\n",
    "print(df.Car)\n",
    "print(\"*\"*20)\n",
    "df.select(df.Car).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0259f3-abfa-4f6b-aa72-72cd3552f3e4",
   "metadata": {},
   "source": [
    "**NOTA:**\n",
    "\n",
    "> **No siempre podemos usar la notación de puntos porque esto se romperá cuando los nombres de las columnas tengan nombres o atributos reservados para la clase del marco de datos. Además, los nombres de las columnas distinguen entre mayúsculas y minúsculas, por lo que siempre debemos asegurarnos de que los nombres de las columnas se hayan cambiado a un caso particular antes de usarlo. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75d216-8c2d-446e-9df5-c20094cf7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "# Column name is case insensitive here\n",
    "print(df['car'])\n",
    "print(\"*\"*20)\n",
    "df.select(df['car']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d56f5d-8a06-432c-99fb-b8f1731f3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "# Column name is case insensitive here\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col('car')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675b1e5-580e-4acb-bf74-1a577fed9521",
   "metadata": {},
   "source": [
    "<a id='seleccionando-múltiples-columnas'></a>\n",
    "### Selección de varias columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a85c0b-99ce-4605-88a8-5655867ac323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st method\n",
    "# Column name is case sensitive in this usage\n",
    "print(df.Car, df.Cylinders)\n",
    "print(\"*\"*40)\n",
    "df.select(df.Car, df.Cylinders).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb966d-c5fa-4670-984a-94332eace2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "# Column name is case insensitive in this usage\n",
    "print(df['car'],df['cylinders'])\n",
    "print(\"*\"*40)\n",
    "df.select(df['car'],df['cylinders']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0973b0-2286-4821-8763-92a5d88e2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "# Column name is case insensitive in this usage\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col('car'),col('cylinders')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8407ef5-b6f8-43e8-a5d7-b28b35d90b06",
   "metadata": {},
   "source": [
    "<a id='agregar-nuevas-columnas'></a>\n",
    "### Adición de nuevas columnas\n",
    "\n",
    "A continuación veremos tres casos:\n",
    "\n",
    "1. Agregar una nueva columna\n",
    "2. Agregar múltiples columnas\n",
    "3. Derivar una nueva columna de una existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925506e2-2ee3-4925-b6ed-edf677f2c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 1: Adding a new column\n",
    "# We will add a new column called 'first_column' at the end\n",
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn('first_column',lit(1)) \n",
    "# lit means literal. It populates the row with the literal value given.\n",
    "# When adding static data / constant values, it is a good practice to use it.\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43c066-d384-4222-9689-c0012e57892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 2: Adding multiple columns\n",
    "# We will add two new columns called 'second_column' and 'third_column' at the end\n",
    "df = df.withColumn('second_column', lit(2)) \\\n",
    "       .withColumn('third_column', lit('Third Column')) \n",
    "# lit means literal. It populates the row with the literal value given.\n",
    "# When adding static data / constant values, it is a good practice to use it.\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373e67-31b6-479c-b418-17ad5e3081e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 3: Deriving a new column from an exisitng one\n",
    "# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n",
    "from pyspark.sql.functions import concat\n",
    "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
    "# lit means literal. It populates the row with the literal value given.\n",
    "# When adding static data / constant values, it is a good practice to use it.\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa9be-df65-447a-aa19-fa62c6904b57",
   "metadata": {},
   "source": [
    "Como podemos ver, el nuevo modelo de coche de columnas se ha creado a partir de columnas existentes. Dado que nuestro objetivo era crear una columna que tuviera el valor del automóvil y el modelo junto con un espacio en el medio, hemos utilizado el operador `concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc324236-cfb2-4617-a3b1-f897a2de32fd",
   "metadata": {},
   "source": [
    "<a id='renombrar-columnas'></a>\n",
    "### Cambio de nombre de columnas\n",
    "\n",
    "Usamos la función `withColumnRenamed` para cambiar el nombre de una columna en PySpark. Veámoslo en acción a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009c639-40b8-4195-b6dc-55058c1be6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming a column in PySpark\n",
    "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
    "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
    "       .withColumnRenamed('third_column', 'new_column_three')\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e198d-8a4f-4dd0-9f74-186a5761e604",
   "metadata": {},
   "source": [
    "<a id='agrupación-por-columnas'></a>\n",
    "### Agrupación por columnas\n",
    "\n",
    "Aquí, vemos la forma de agrupar valores de la API Dataframe. Discutiremos cómo:\n",
    "\n",
    "\n",
    "1. Agrupar por una sola columna\n",
    "2. Agrupar por múltiples columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11ec37-4705-4328-a557-ac4ace3cf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By a column in PySpark\n",
    "df.groupBy('Origin').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e3c6a-f69c-4bc8-a226-d4a260c848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By multiple columns in PySpark\n",
    "df.groupBy('Origin', 'Model').count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fef5e-f502-4f65-9abf-8ad8d941cbd9",
   "metadata": {},
   "source": [
    "<a id='eliminando-columnas'></y>\n",
    "### Eliminación de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df69045-5b3d-4bf0-9627-50e6ca6b94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns in PySpark\n",
    "df = df.drop('new_column_one')\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30ac18-52ae-4879-ba1c-61f2f34fc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove multiple columnss in one go\n",
    "df = df.drop('new_column_two') \\\n",
    "       .drop('new_column_three')\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3182775-2104-4868-a849-d092532100f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
