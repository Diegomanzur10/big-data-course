{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61daad76-a9c0-4762-9cba-608343d67e3f",
   "metadata": {},
   "source": [
    "## Spark SQL es muy fácil de usar, y punto. Quizá ya sepas que también es bastante difícil de dominar.\n",
    "\n",
    "Para ser competente en Spark, hay que tener tres habilidades fundamentales:\n",
    "\n",
    "-La capacidad de manipular y entender los datos\n",
    "-El conocimiento de cómo adaptar la herramienta a las necesidades del programador\n",
    "-El arte de encontrar un equilibrio entre los factores que afectan a las ejecuciones de los trabajos de Spark\n",
    "\n",
    "He elaborado los siguientes seis ejercicios que se asemejan a algunas situaciones típicas a las que los desarrolladores de Spark se enfrentan a diario cuando construyen sus pipelines: estos ayudarán a evaluar las habilidades anteriores.\n",
    "\n",
    "### El conjunto de datos\n",
    "Describamos brevemente el conjunto de datos que vamos a utilizar: consta de tres tablas procedentes de la base de datos de una tienda, con productos, ventas y vendedores. Los datos están disponibles en archivos Parquet\n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/700/1*wA4xJu3LMcm_vR5pFJkLpA.png\" width=500 height=500 />\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bfed4-9d24-4c7f-bd64-30e59b21ffd8",
   "metadata": {},
   "source": [
    "### Tabla de ventas\n",
    "Cada fila de esta tabla es un pedido y cada pedido puede contener sólo un producto. Cada fila almacena los siguientes campos:\n",
    "\n",
    "- order_id: El ID del pedido\n",
    "- product_id: El único producto vendido en el pedido. Todos los pedidos tienen exactamente un producto)\n",
    "- seller_id: El ID del empleado vendedor que vendió el producto\n",
    "- num_pieces_sold: El número de unidades vendidas para el producto específico en el pedido\n",
    "- bill_raw_text: Una cadena que representa el texto en bruto de la factura asociada al pedido\n",
    "- date: La fecha del pedido.\n",
    "\n",
    "### Tabla de productos\n",
    "Cada fila representa un producto distinto. Los campos son\n",
    "\n",
    "- product_id: El ID del producto\n",
    "- product_name: El nombre del producto\n",
    "- price: El precio del producto\n",
    "\n",
    "### Tabla de vendedores\n",
    "Esta tabla contiene la lista de todos los vendedores:\n",
    "\n",
    "- seller_id: El ID del vendedor\n",
    "- seller_name: El nombre del vendedor\n",
    "- daily_target: El número de artículos (independientemente del tipo de producto) que el vendedor necesita para alcanzar su cuota. Por ejemplo, si el objetivo diario es 100.000, el empleado necesita vender 100.000 productos, puede alcanzar la cuota vendiendo 100.000 unidades del producto_0, pero también vendiendo 30.000 unidades del producto_1 y 70.000 unidades del producto_2\n",
    "\n",
    "\n",
    "### Ejercicios\n",
    "La mejor manera de aprovechar los ejercicios que se presentan a continuación es obtener los datos e implementar un código de trabajo que resuelva los problemas propuestos.\n",
    "\n",
    "Consejo: He construido el conjunto de datos para poder trabajar en una sola máquina: cuando escribas el código, imagina lo que pasaría con un conjunto de datos 100 veces mayor.\n",
    "\n",
    "Aunque sepas cómo resolverlas, ¡mi consejo es que no te saltes las preguntas de calentamiento! (si sabes que Spark te llevará unos segundos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d24e3e-0c5f-4bb2-bda3-1cfca0ce90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark-3.2.1-bin-hadoop3.2/python (3.2.1)\n",
      "Collecting pyspark-stubs\n",
      "  Downloading pyspark_stubs-3.0.0.post3-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/110.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyspark\n",
      "  Downloading pyspark-3.0.3.tar.gz (209.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.1/209.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-3.0.2.tar.gz (204.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyspark-stubs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyspark-stubs\n",
      "  Downloading pyspark_stubs-3.0.0.post2-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/110.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-3.0.0.post1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-3.0.0-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post12-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyspark\n",
      "  Downloading pyspark-2.4.8.tar.gz (220.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.5/220.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.7.tar.gz (217.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.9/217.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.6.tar.gz (218.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.4/218.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.4.tar.gz (215.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.3.tar.gz (215.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.2.tar.gz (193.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.1.tar.gz (215.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.4.0.tar.gz (213.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyspark-stubs\n",
      "  Downloading pyspark_stubs-2.4.0.post11-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m275.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post10-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m313.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post9-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m341.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyspark-stubs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pyspark_stubs-2.4.0.post8-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m444.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post7-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m280.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post6-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m221.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post5-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m275.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post4-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m222.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pyspark_stubs-2.4.0.post3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post2-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0.post1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.4.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.3.0.post7-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyspark\n",
      "  Downloading pyspark-2.3.4.tar.gz (212.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.3.3.tar.gz (211.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.9/211.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.3.2.tar.gz (211.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.9/211.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.3.1.tar.gz (211.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.9/211.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading pyspark-2.3.0.tar.gz (211.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.9/211.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyspark-stubs\n",
      "  Downloading pyspark_stubs-2.3.0.post6-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m236.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.3.0.post5-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m239.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.3.0.post4-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m206.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.3.0.post3-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pyspark_stubs-2.3.0.post2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, pyspark-stubs\n",
      "Successfully installed py4j-0.10.9.3 pyspark-stubs-2.3.0.post2\n"
     ]
    }
   ],
   "source": [
    "# Pyspark\n",
    "!pip install pyspark pyspark-stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3edddc-0499-4553-896b-de19e37904d2",
   "metadata": {},
   "source": [
    "### 1.0 ¿Cuál es el ingreso medio de los pedidos?\n",
    "\n",
    "Recuerde que los ingresos = precio * cantidad. (revenue = price * quantity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28b0c2-33e1-422a-91da-e5d013c37b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1245.9236386027228|\n",
      "+------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Solución ejercicio 1.0\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.files.overwrite\", \"true\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "products = spark.read.csv(\n",
    "   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sellers = spark.read.csv(\n",
    "   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sales = spark.read.csv(\n",
    "   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "##### A partir de aquí escriba su respuesta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3cf2a-26ff-4d71-b46d-738a43c21edd",
   "metadata": {},
   "source": [
    "### 2.0  Para cada vendedor, ¿cuál es el porcentaje medio de contribución de un pedido a la cuota diaria del vendedor?\n",
    "\n",
    "#### Ejemplo\n",
    "Si el Seller_0 con `quota=250`  tiene 3 pedidos:\n",
    "\n",
    "- Order  1: 10 productos vendidos\n",
    "- Order  2: 8 productos vendidos\n",
    "- Order  3: 7 productos vendidos\n",
    "\n",
    "El porcentaje medio de contribución de los pedidos a la cuota del vendedor sería\n",
    "- Order  1: 10/250 = 0,04\n",
    "- Order  2: 8/250 = 0,032\n",
    "- Order  3: 7/250 = 0,028\n",
    "\n",
    "Porcentaje medio de contribución = (0,04+0,032+0,028)/3 = 0,03333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6029b64-3769-4072-9c37-6f656f2e81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        3|7.060842894390445E-4|\n",
      "|        7|8.510553537464244E-5|\n",
      "|        8|0.002071646546208...|\n",
      "|        0|2.019736225290412...|\n",
      "|        1|3.670188787905763...|\n",
      "|        2|2.456721945951509E-4|\n",
      "|        4|3.845384604576898...|\n",
      "|        5|8.038980497173663E-5|\n",
      "|        6|2.534518215186249...|\n",
      "|        9|1.449276275189615E-4|\n",
      "+---------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Solución ejercicio 2.0\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.files.overwrite\", \"true\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "products = spark.read.csv(\n",
    "   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sellers = spark.read.csv(\n",
    "   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sales = spark.read.csv(\n",
    "   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "##### A partir de aquí escriba su respuesta\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b908e-f75c-464b-98b7-8002f2a2e26a",
   "metadata": {},
   "source": [
    "### 3.0  ¿Quiénes son las segundas personas (vendedores) que más venden y las que menos venden para cada producto?\n",
    "\n",
    "Recomendación: use la función `Window` [link de ayuda](https://sparkbyexamples.com/spark/spark-sql-window-functions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a8f98-6f11-4d0f-81f2-bdc72186efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|product_id|seller_id|             type|\n",
      "+----------+---------+-----------------+\n",
      "|  19581081|        1|     Least Seller|\n",
      "|   3093518|        1|     Least Seller|\n",
      "|   3147422|        1|     Least Seller|\n",
      "|   3384033|        1|     Least Seller|\n",
      "|  57452470|        2|     Least Seller|\n",
      "|  25078796|        4|     Least Seller|\n",
      "|  42503932|        4|     Least Seller|\n",
      "|  49196172|        4|     Least Seller|\n",
      "|  70135114|        4|     Least Seller|\n",
      "|  24992315|        5|     Least Seller|\n",
      "|  46764734|        5|     Least Seller|\n",
      "|  36289301|        6|     Least Seller|\n",
      "|  55049289|        6|     Least Seller|\n",
      "|  57356373|        6|     Least Seller|\n",
      "|  22113759|        7|     Least Seller|\n",
      "|  44179664|        7|     Least Seller|\n",
      "|   5342885|        7|     Least Seller|\n",
      "|  54810407|        7|     Least Seller|\n",
      "|  74608973|        7|     Least Seller|\n",
      "|  19657520|        8|     Least Seller|\n",
      "|  63583705|        8|     Least Seller|\n",
      "|  28365122|        9|     Least Seller|\n",
      "|  10005339|        4|Second top seller|\n",
      "|  10016111|        4|Second top seller|\n",
      "|  10019294|        4|Second top seller|\n",
      "|  10060162|        8|Second top seller|\n",
      "|  10062373|        3|Second top seller|\n",
      "|  10105459|        4|Second top seller|\n",
      "|  10109807|        3|Second top seller|\n",
      "|   1014590|        2|Second top seller|\n",
      "|  10146669|        3|Second top seller|\n",
      "|   1014907|        9|Second top seller|\n",
      "|  10164466|        5|Second top seller|\n",
      "|   1017560|        7|Second top seller|\n",
      "|  10188212|        5|Second top seller|\n",
      "|  10193037|        5|Second top seller|\n",
      "|  10206662|        7|Second top seller|\n",
      "|  10207764|        6|Second top seller|\n",
      "|  10215508|        4|Second top seller|\n",
      "|  10221292|        9|Second top seller|\n",
      "|  10230212|        1|Second top seller|\n",
      "|  10244106|        7|Second top seller|\n",
      "|  10251355|        8|Second top seller|\n",
      "|  10267458|        2|Second top seller|\n",
      "|  10279410|        6|Second top seller|\n",
      "|  10279663|        1|Second top seller|\n",
      "|  10281152|        3|Second top seller|\n",
      "|  10281157|        1|Second top seller|\n",
      "|   1028287|        6|Second top seller|\n",
      "|  10294173|        4|Second top seller|\n",
      "|  10310098|        5|Second top seller|\n",
      "|   1032843|        2|Second top seller|\n",
      "|  10348892|        5|Second top seller|\n",
      "|   1034952|        3|Second top seller|\n",
      "|  10360797|        1|Second top seller|\n",
      "|   1040085|        5|Second top seller|\n",
      "|  10427416|        8|Second top seller|\n",
      "|  10436328|        6|Second top seller|\n",
      "|  10439000|        4|Second top seller|\n",
      "|  10446190|        6|Second top seller|\n",
      "|  10454042|        5|Second top seller|\n",
      "|  10458120|        1|Second top seller|\n",
      "|  10459456|        3|Second top seller|\n",
      "|  10481850|        5|Second top seller|\n",
      "|  10484060|        4|Second top seller|\n",
      "|  10492249|        6|Second top seller|\n",
      "|  10502152|        3|Second top seller|\n",
      "|  10515671|        8|Second top seller|\n",
      "|  10529321|        8|Second top seller|\n",
      "|  10533297|        3|Second top seller|\n",
      "|  10546974|        1|Second top seller|\n",
      "|  10549754|        2|Second top seller|\n",
      "|   1055806|        6|Second top seller|\n",
      "|  10563618|        7|Second top seller|\n",
      "|  10572909|        8|Second top seller|\n",
      "|  10579353|        9|Second top seller|\n",
      "|  10586734|        4|Second top seller|\n",
      "|  10587105|        7|Second top seller|\n",
      "|  10590375|        3|Second top seller|\n",
      "|  10590801|        9|Second top seller|\n",
      "|  10610025|        2|Second top seller|\n",
      "|  10615573|        7|Second top seller|\n",
      "|   1064190|        7|Second top seller|\n",
      "|  10651039|        2|Second top seller|\n",
      "|   1065268|        6|Second top seller|\n",
      "|  10670709|        2|Second top seller|\n",
      "|  10673860|        6|Second top seller|\n",
      "|  10684919|        3|Second top seller|\n",
      "|  10692622|        5|Second top seller|\n",
      "|  10695459|        2|Second top seller|\n",
      "|  10724237|        7|Second top seller|\n",
      "|  10724587|        2|Second top seller|\n",
      "|  10736153|        5|Second top seller|\n",
      "|  10748044|        6|Second top seller|\n",
      "|  10752694|        9|Second top seller|\n",
      "|    107572|        5|Second top seller|\n",
      "|  10758434|        4|Second top seller|\n",
      "|  10817798|        1|Second top seller|\n",
      "|  10832434|        1|Second top seller|\n",
      "|   1083627|        3|Second top seller|\n",
      "+----------+---------+-----------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solución ejercicio 3.0\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.files.overwrite\", \"true\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "products = spark.read.csv(\n",
    "   'file:///clase8/products.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sellers = spark.read.csv(\n",
    "   'file:///clase8/sellers.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "sales = spark.read.csv(\n",
    "   'file:///clase8/sales.csv', header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "##### A partir de aquí escriba su respuesta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef68608-971c-4c47-825b-26588dee5130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0debd-2df0-4abe-b3d8-1a8fbc9ccfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a857c-9727-476a-b5e8-6cf58fdfadd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3959890-240f-42ba-a15c-fdd103739f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888392b-01b2-4fe1-b196-7bcc5f67e0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87b54e-9349-4fa3-8c36-a8d8e992f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fb0d4-3b1c-46fa-ae7a-6410a82a9885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e56055-1886-45a8-bb70-5f8af8a5435c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4741c4-b3e2-40b6-a464-e0c1151562ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61badb22-0631-475e-97a7-f11da09403bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204644ee-0221-4d71-86fc-d77eeba81751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6f81a-e1c7-40e1-bf0c-1a4422ee0d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7a134-d548-4bf2-80bf-a1ca16550549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ed003-64a1-4045-9601-5d38d778b6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7a5d1-d5c7-4e70-94f0-872693276f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2878ba-c937-4cc2-8ba9-f03e12f8d64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae666dbd-9a2d-4a04-869d-ee811368a137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaa428-eb94-4fb1-a7fa-79c8c74d8057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c6fe6-eef8-45b9-bc1d-38c86f5bc96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
